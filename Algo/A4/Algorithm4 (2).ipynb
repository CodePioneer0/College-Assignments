{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc-I70XM4HqW"
      },
      "source": [
        "DATASET 1 : STUDENT PERFORMANCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57q8dxXu35bW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "student_performance = fetch_ucirepo(id=320)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = student_performance.data.features\n",
        "y = student_performance.data.targets\n",
        "\n",
        "# Create a copy of X and y to avoid the SettingWithCopyWarning\n",
        "X_copy = X.copy()\n",
        "y_copy = y.copy()\n",
        "\n",
        "# Check for missing values before preprocessing\n",
        "print(f\"Missing values in original X: {X_copy.isnull().sum().sum()}\")\n",
        "print(f\"Missing values in original y: {y_copy.isnull().sum().sum()}\")\n",
        "\n",
        "# Handle missing values properly\n",
        "X_copy = X_copy.replace('?', np.nan)\n",
        "X_copy = X_copy.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill remaining missing values with the mean of each column\n",
        "for col in X_copy.columns:\n",
        "    if X_copy[col].isnull().sum() > 0:\n",
        "        X_copy[col] = X_copy[col].fillna(X_copy[col].mean())\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(f\"Missing values after preprocessing: {X_copy.isnull().sum().sum()}\")\n",
        "\n",
        "# For the student performance dataset, we need to select a target variable\n",
        "# Let's use G3 (final grade) as our primary target\n",
        "if 'G3' in y_copy.columns:\n",
        "    target = y_copy['G3']\n",
        "else:\n",
        "    # If G3 doesn't exist, use the first target column\n",
        "    target = y_copy.iloc[:, 0]\n",
        "\n",
        "# Make sure target has no missing values\n",
        "target = target.fillna(target.mean())\n",
        "\n",
        "# Convert to classification problem (since the original code uses classification metrics)\n",
        "# Assuming grades below 10 are failing (standard in many grading systems)\n",
        "target_class = (target >= 10).astype(int)\n",
        "\n",
        "# Standardizing features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_copy)\n",
        "\n",
        "# Double-check for NaN values after scaling\n",
        "if np.isnan(X_scaled).any():\n",
        "    print(\"Warning: NaN values found after scaling. Replacing with 0...\")\n",
        "    X_scaled = np.nan_to_num(X_scaled)\n",
        "\n",
        "# Splitting data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, target_class, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression as the base classifier\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Function to evaluate and collect performance metrics\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"f1_score\": f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    }\n",
        "\n",
        "# Classifiers to evaluate\n",
        "classifiers = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"ANN\": MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "# Dimensionality reduction methods\n",
        "methods = {}\n",
        "# Test a range of dimensions\n",
        "for d in range(1, min(11, X_copy.shape[1])):  # Up to 10 features or max available\n",
        "    print(f\"\\n### Feature Selection and Dimensionality Reduction with d = {d} ###\\n\")\n",
        "\n",
        "    # Naïve Search (Random selection of d features)\n",
        "    feature_indices = np.random.choice(range(X_scaled.shape[1]), d, replace=False)\n",
        "    X_train_naive = X_train[:, feature_indices]\n",
        "    X_test_naive = X_test[:, feature_indices]\n",
        "    print(f\"Naïve Search Selected Features: {feature_indices}\")\n",
        "    methods[f\"Naive_{d}\"] = (X_train_naive, X_test_naive)\n",
        "\n",
        "    try:\n",
        "        # Step-wise Forward Selection\n",
        "        sfs_forward = SequentialFeatureSelector(model, n_features_to_select=d, direction='forward').fit(X_train, y_train)\n",
        "        X_train_sfs = sfs_forward.transform(X_train)\n",
        "        X_test_sfs = sfs_forward.transform(X_test)\n",
        "        print(f\"Step-wise Forward Selection Selected Features: {np.where(sfs_forward.get_support())[0]}\")\n",
        "        methods[f\"SFS_Forward_{d}\"] = (X_train_sfs, X_test_sfs)\n",
        "    except Exception as e:\n",
        "        print(f\"Step-wise Forward Selection failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        # Step-wise Backward Removal\n",
        "        sfs_backward = SequentialFeatureSelector(model, n_features_to_select=d, direction='backward').fit(X_train, y_train)\n",
        "        X_train_sbs = sfs_backward.transform(X_train)\n",
        "        X_test_sbs = sfs_backward.transform(X_test)\n",
        "        print(f\"Step-wise Backward Removal Selected Features: {np.where(sfs_backward.get_support())[0]}\")\n",
        "        methods[f\"SFS_Backward_{d}\"] = (X_train_sbs, X_test_sbs)\n",
        "    except Exception as e:\n",
        "        print(f\"Step-wise Backward Removal failed: {e}\")\n",
        "\n",
        "    # Bidirectional Search (using MLxtend's SFS)\n",
        "    try:\n",
        "        sfs_bi = SFS(model, k_features=d, forward=True, floating=False, scoring='accuracy', cv=5).fit(X_train, y_train)\n",
        "        X_train_bi = X_train[:, list(sfs_bi.k_feature_idx_)]\n",
        "        X_test_bi = X_test[:, list(sfs_bi.k_feature_idx_)]\n",
        "        print(f\"Bidirectional Search Selected Features: {list(sfs_bi.k_feature_idx_)}\")\n",
        "        methods[f\"SFS_Bi_{d}\"] = (X_train_bi, X_test_bi)\n",
        "    except Exception as e:\n",
        "        print(f\"Bidirectional Search failed: {e}\")\n",
        "\n",
        "    # Step-wise Floating Forward Selection\n",
        "    try:\n",
        "        sfs_float_forward = SFS(model, k_features=d, forward=True, floating=True, scoring='accuracy', cv=5).fit(X_train, y_train)\n",
        "        X_train_sfsf = X_train[:, list(sfs_float_forward.k_feature_idx_)]\n",
        "        X_test_sfsf = X_test[:, list(sfs_float_forward.k_feature_idx_)]\n",
        "        print(f\"Step-wise Floating Forward Selection Selected Features: {list(sfs_float_forward.k_feature_idx_)}\")\n",
        "        methods[f\"SFS_Float_Forward_{d}\"] = (X_train_sfsf, X_test_sfsf)\n",
        "    except Exception as e:\n",
        "        print(f\"Floating Forward Selection failed: {e}\")\n",
        "\n",
        "    # Step-wise Floating Backward Removal\n",
        "    try:\n",
        "        sfs_float_backward = SFS(model, k_features=d, forward=False, floating=True, scoring='accuracy', cv=5).fit(X_train, y_train)\n",
        "        X_train_sfsb = X_train[:, list(sfs_float_backward.k_feature_idx_)]\n",
        "        X_test_sfsb = X_test[:, list(sfs_float_backward.k_feature_idx_)]\n",
        "        print(f\"Step-wise Floating Backward Removal Selected Features: {list(sfs_float_backward.k_feature_idx_)}\")\n",
        "        methods[f\"SFS_Float_Backward_{d}\"] = (X_train_sfsb, X_test_sfsb)\n",
        "    except Exception as e:\n",
        "        print(f\"Floating Backward Selection failed: {e}\")\n",
        "\n",
        "    # Principal Component Analysis (PCA)\n",
        "    try:\n",
        "        pca = PCA(n_components=d)\n",
        "        X_train_pca = pca.fit_transform(X_train)\n",
        "        X_test_pca = pca.transform(X_test)\n",
        "        print(f\"PCA Explained Variance Ratio: {pca.explained_variance_ratio_}\")\n",
        "        methods[f\"PCA_{d}\"] = (X_train_pca, X_test_pca)\n",
        "    except Exception as e:\n",
        "        print(f\"PCA failed: {e}\")\n",
        "\n",
        "    # Linear Discriminant Analysis (LDA)\n",
        "    try:\n",
        "        n_components = min(d, len(np.unique(y_train)) - 1)  # LDA components <= classes - 1\n",
        "        if n_components > 0:  # Only perform LDA if we have enough classes\n",
        "            lda = LDA(n_components=n_components)\n",
        "            X_train_lda = lda.fit_transform(X_train, y_train)\n",
        "            X_test_lda = lda.transform(X_test)\n",
        "            if hasattr(lda, 'explained_variance_ratio_'):\n",
        "                print(f\"LDA Explained Variance Ratio: {lda.explained_variance_ratio_}\")\n",
        "            methods[f\"LDA_{d}\"] = (X_train_lda, X_test_lda)\n",
        "    except Exception as e:\n",
        "        print(f\"LDA failed: {e}\")\n",
        "\n",
        "# Evaluate classifiers on each method\n",
        "results = {}\n",
        "for method_name, (X_tr, X_te) in methods.items():\n",
        "    results[method_name] = {}\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        try:\n",
        "            results[method_name][clf_name] = evaluate_model(clf, X_tr, X_te, y_train, y_test)\n",
        "            print(f\"Evaluated {clf_name} with {method_name}: {results[method_name][clf_name]['accuracy']:.4f} accuracy\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating {clf_name} with {method_name}: {e}\")\n",
        "            results[method_name][clf_name] = {\"accuracy\": 0, \"precision\": 0, \"recall\": 0, \"f1_score\": 0}\n",
        "\n",
        "# Convert results to DataFrame for analysis\n",
        "results_df = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
        "                                   for i in results.keys()\n",
        "                                   for j in results[i].keys()},\n",
        "                                   orient='index')\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results_df.to_csv(\"student_performance_metrics.csv\")\n",
        "\n",
        "print(\"Feature selection and dimensionality reduction completed.\")\n",
        "print(\"Results saved to 'student_performance_metrics.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1AjYl7O38qG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"f1_score\": f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    }\n",
        "\n",
        "# Classifiers to evaluate\n",
        "classifiers = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"ANN\": MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "# Evaluate classifiers on each method\n",
        "results = {}\n",
        "for method_name, (X_tr, X_te) in methods.items():\n",
        "    results[method_name] = {}\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        try:\n",
        "            results[method_name][clf_name] = evaluate_model(clf, X_tr, X_te, y_train, y_test)\n",
        "            print(f\"Evaluated {clf_name} with {method_name}: {results[method_name][clf_name]['accuracy']:.4f} accuracy\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating {clf_name} with {method_name}: {e}\")\n",
        "            results[method_name][clf_name] = {\"accuracy\": 0, \"precision\": 0, \"recall\": 0, \"f1_score\": 0}\n",
        "\n",
        "# Convert results to DataFrame for analysis\n",
        "results_df = pd.DataFrame.from_dict({(i, j): results[i][j] for i in results.keys() for j in results[i].keys()},\n",
        "                                   orient='index')\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results_df.to_csv(\"student_performance_metrics.csv\")\n",
        "\n",
        "print(\"Model evaluation completed. Results saved to 'student_performance_metrics.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd26o2LY4GSk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the performance metrics data\n",
        "results_df = pd.read_csv(\"student_performance_metrics.csv\", index_col=[0, 1])\n",
        "\n",
        "# Ensure the index is a MultiIndex\n",
        "if not isinstance(results_df.index, pd.MultiIndex):\n",
        "    results_df.index = pd.MultiIndex.from_tuples(results_df.index)\n",
        "\n",
        "# Check the structure of the DataFrame\n",
        "print(\"DataFrame Structure:\")\n",
        "print(results_df.index)\n",
        "print(results_df.columns)\n",
        "\n",
        "# 4i) Table Summary\n",
        "print(\"Table Summary of Performance Metrics:\")\n",
        "print(results_df)\n",
        "\n",
        "# 4ii) Pie Charts\n",
        "def plot_pie_charts(results_df, metric):\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(18, 18))\n",
        "    fig.suptitle(f'Pie Charts for {metric.capitalize()}', fontsize=20)\n",
        "    for idx, (method, data) in enumerate(results_df.groupby(level=0)):\n",
        "        if idx >= 9:\n",
        "            break  # Ensure we don't exceed subplot grid\n",
        "        ax = axes[idx // 3, idx % 3]\n",
        "        data = data.xs(method, level=0)[metric]  # Access data with xs using level=0\n",
        "        data.plot.pie(ax=ax, autopct='%1.1f%%', title=f'{method}')\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_pie_charts(results_df, metric)\n",
        "\n",
        "# 4iii) Bar Charts\n",
        "def plot_bar_charts(results_df, metric):\n",
        "    results_metric = results_df[metric]  # Directly index if it's not a MultiIndex\n",
        "    results_metric.unstack(level=1).plot(kind='bar', figsize=(15, 10), title=f'Bar Charts for {metric.capitalize()}')\n",
        "    plt.ylabel(metric)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Classifiers', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_bar_charts(results_df, metric)\n",
        "\n",
        "# 4iv) Line Graphs\n",
        "def plot_line_graphs(results_df, metric):\n",
        "    results_metric = results_df[metric]  # Directly index if it's not a MultiIndex\n",
        "    results_metric.unstack(level=1).plot(kind='line', marker='o', figsize=(15, 10), title=f'Line Graphs for {metric.capitalize()}')\n",
        "    plt.ylabel(metric)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Classifiers', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_line_graphs(results_df, metric)\n",
        "\n",
        "# 4vi) Box Plots\n",
        "def plot_box_plots(results_df, metric):\n",
        "    results_metric = results_df[metric]  # Directly index if it's not a MultiIndex\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    sns.boxplot(data=results_metric.unstack(level=1))\n",
        "    plt.title(f'Box Plots for {metric.capitalize()}')\n",
        "    plt.ylabel(metric)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_box_plots(results_df, metric)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiOeAhuaFkN6"
      },
      "source": [
        "DATASET 2 : WINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzHXkfdBF2-h",
        "outputId": "6fb249f1-c296-4f4e-963e-5bf053cbfc7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'uci_id': 109, 'name': 'Wine', 'repository_url': 'https://archive.ics.uci.edu/dataset/109/wine', 'data_url': 'https://archive.ics.uci.edu/static/public/109/data.csv', 'abstract': 'Using chemical analysis to determine the origin of wines', 'area': 'Physics and Chemistry', 'tasks': ['Classification'], 'characteristics': ['Tabular'], 'num_instances': 178, 'num_features': 13, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1992, 'last_updated': 'Mon Aug 28 2023', 'dataset_doi': '10.24432/C5PC7J', 'creators': ['Stefan Aeberhard', 'M. Forina'], 'intro_paper': {'ID': 246, 'type': 'NATIVE', 'title': 'Comparative analysis of statistical pattern recognition methods in high dimensional settings', 'authors': 'S. Aeberhard, D. Coomans, O. Vel', 'venue': 'Pattern Recognition', 'year': 1994, 'journal': None, 'DOI': '10.1016/0031-3203(94)90145-7', 'URL': 'https://www.semanticscholar.org/paper/83dc3e4030d7b9fbdbb4bde03ce12ab70ca10528', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. \\r\\n\\r\\nI think that the initial data set had around 30 variables, but for some reason I only have the 13 dimensional version. I had a list of what the 30 or so variables were, but a.)  I lost it, and b.), I would not know which 13 variables are included in the set.\\r\\n\\r\\nThe attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it )\\r\\n1) Alcohol\\r\\n2) Malic acid\\r\\n3) Ash\\r\\n4) Alcalinity of ash  \\r\\n5) Magnesium\\r\\n6) Total phenols\\r\\n7) Flavanoids\\r\\n8) Nonflavanoid phenols\\r\\n9) Proanthocyanins\\r\\n10)Color intensity\\r\\n11)Hue\\r\\n12)OD280/OD315 of diluted wines\\r\\n13)Proline \\r\\n\\r\\nIn a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging.           ', 'purpose': 'test', 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'All attributes are continuous\\r\\n\\t\\r\\nNo statistics available, but suggest to standardise variables for certain uses (e.g. for us with classifiers which are NOT scale invariant)\\r\\n\\r\\nNOTE: 1st attribute is class identifier (1-3)', 'citation': None}}\n",
            "                            name     role         type demographic  \\\n",
            "0                          class   Target  Categorical        None   \n",
            "1                        Alcohol  Feature   Continuous        None   \n",
            "2                      Malicacid  Feature   Continuous        None   \n",
            "3                            Ash  Feature   Continuous        None   \n",
            "4              Alcalinity_of_ash  Feature   Continuous        None   \n",
            "5                      Magnesium  Feature      Integer        None   \n",
            "6                  Total_phenols  Feature   Continuous        None   \n",
            "7                     Flavanoids  Feature   Continuous        None   \n",
            "8           Nonflavanoid_phenols  Feature   Continuous        None   \n",
            "9                Proanthocyanins  Feature   Continuous        None   \n",
            "10               Color_intensity  Feature   Continuous        None   \n",
            "11                           Hue  Feature   Continuous        None   \n",
            "12  0D280_0D315_of_diluted_wines  Feature   Continuous        None   \n",
            "13                       Proline  Feature      Integer        None   \n",
            "\n",
            "   description units missing_values  \n",
            "0         None  None             no  \n",
            "1         None  None             no  \n",
            "2         None  None             no  \n",
            "3         None  None             no  \n",
            "4         None  None             no  \n",
            "5         None  None             no  \n",
            "6         None  None             no  \n",
            "7         None  None             no  \n",
            "8         None  None             no  \n",
            "9         None  None             no  \n",
            "10        None  None             no  \n",
            "11        None  None             no  \n",
            "12        None  None             no  \n",
            "13        None  None             no  \n",
            "Missing values in original X: 0\n",
            "Missing values in original y: 0\n",
            "Missing values after preprocessing: 0\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 1 ###\n",
            "\n",
            "Naïve Search Selected Features: [4]\n",
            "Step-wise Forward Selection Selected Features: [6]\n",
            "Step-wise Backward Removal Selected Features: [0]\n",
            "Bidirectional Search Selected Features: [6]\n",
            "Step-wise Floating Forward Selection Selected Features: [6]\n",
            "Step-wise Floating Backward Removal Selected Features: [6]\n",
            "PCA Explained Variance Ratio: [0.3587757]\n",
            "LDA Explained Variance Ratio: [0.71429654]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 2 ###\n",
            "\n",
            "Naïve Search Selected Features: [7 9]\n",
            "Step-wise Forward Selection Selected Features: [6 9]\n",
            "Step-wise Backward Removal Selected Features: [ 0 10]\n",
            "Bidirectional Search Selected Features: [6, 9]\n",
            "Step-wise Floating Forward Selection Selected Features: [6, 9]\n",
            "Step-wise Floating Backward Removal Selected Features: [0, 6]\n",
            "PCA Explained Variance Ratio: [0.3587757  0.18347789]\n",
            "LDA Explained Variance Ratio: [0.71429654 0.28570346]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 3 ###\n",
            "\n",
            "Naïve Search Selected Features: [9 6 3]\n",
            "Step-wise Forward Selection Selected Features: [ 6  9 12]\n",
            "Step-wise Backward Removal Selected Features: [ 0 10 12]\n",
            "Bidirectional Search Selected Features: [6, 9, 12]\n",
            "Step-wise Floating Forward Selection Selected Features: [6, 9, 12]\n",
            "Step-wise Floating Backward Removal Selected Features: [0, 6, 12]\n",
            "PCA Explained Variance Ratio: [0.3587757  0.18347789 0.11776451]\n",
            "LDA Explained Variance Ratio: [0.71429654 0.28570346]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 4 ###\n",
            "\n",
            "Naïve Search Selected Features: [1 4 7 9]\n",
            "Step-wise Forward Selection Selected Features: [ 0  6  9 12]\n",
            "Step-wise Backward Removal Selected Features: [ 0  2 10 12]\n",
            "Bidirectional Search Selected Features: [0, 6, 9, 12]\n",
            "Step-wise Floating Forward Selection Selected Features: [0, 6, 9, 12]\n",
            "Step-wise Floating Backward Removal Selected Features: [0, 2, 6, 12]\n",
            "PCA Explained Variance Ratio: [0.3587757  0.18347789 0.11776451 0.07464401]\n",
            "LDA Explained Variance Ratio: [0.71429654 0.28570346]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 5 ###\n",
            "\n",
            "Naïve Search Selected Features: [ 2 12  8 11 10]\n",
            "Step-wise Forward Selection Selected Features: [ 0  1  6  9 12]\n",
            "Step-wise Backward Removal Selected Features: [ 0  2  8 10 12]\n",
            "Bidirectional Search Selected Features: [0, 1, 6, 9, 12]\n",
            "Step-wise Floating Forward Selection Selected Features: [0, 1, 6, 9, 12]\n",
            "Step-wise Floating Backward Removal Selected Features: [0, 2, 8, 10, 12]\n",
            "PCA Explained Variance Ratio: [0.3587757  0.18347789 0.11776451 0.07464401 0.06720632]\n",
            "LDA Explained Variance Ratio: [0.71429654 0.28570346]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 6 ###\n",
            "\n",
            "Naïve Search Selected Features: [ 0  2  5  3 10  4]\n",
            "Step-wise Forward Selection Selected Features: [ 0  1  2  6  9 12]\n",
            "Step-wise Backward Removal Selected Features: [ 0  2  3  8 10 12]\n",
            "Bidirectional Search Selected Features: [0, 1, 2, 6, 9, 12]\n",
            "Step-wise Floating Forward Selection Selected Features: [0, 1, 2, 6, 9, 12]\n",
            "Step-wise Floating Backward Removal Selected Features: [0, 2, 3, 8, 10, 12]\n",
            "PCA Explained Variance Ratio: [0.3587757  0.18347789 0.11776451 0.07464401 0.06720632 0.04898502]\n",
            "LDA Explained Variance Ratio: [0.71429654 0.28570346]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 7 ###\n",
            "\n",
            "Naïve Search Selected Features: [ 6  4  1  9  5  7 11]\n",
            "Step-wise Forward Selection Selected Features: [ 0  1  2  3  6  9 12]\n",
            "Step-wise Backward Removal Selected Features: [ 0  2  3  6  8 10 12]\n",
            "Bidirectional Search Selected Features: [0, 1, 2, 3, 6, 9, 12]\n",
            "Step-wise Floating Forward Selection Selected Features: [0, 1, 2, 3, 6, 9, 12]\n",
            "Step-wise Floating Backward Removal Selected Features: [0, 2, 3, 6, 8, 10, 12]\n",
            "PCA Explained Variance Ratio: [0.3587757  0.18347789 0.11776451 0.07464401 0.06720632 0.04898502\n",
            " 0.0428154 ]\n",
            "LDA Explained Variance Ratio: [0.71429654 0.28570346]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 8 ###\n",
            "\n",
            "Naïve Search Selected Features: [ 9  6  2  0  3  7 11  1]\n",
            "Step-wise Forward Selection Selected Features: [ 0  1  2  3  4  6  9 12]\n",
            "Step-wise Backward Removal Selected Features: [ 0  2  3  6  8  9 10 12]\n",
            "Bidirectional Search Selected Features: [0, 1, 2, 3, 4, 6, 9, 12]\n",
            "Step-wise Floating Forward Selection Selected Features: [0, 1, 2, 3, 4, 6, 9, 12]\n",
            "Step-wise Floating Backward Removal Selected Features: [0, 2, 3, 6, 9, 10, 11, 12]\n",
            "PCA Explained Variance Ratio: [0.3587757  0.18347789 0.11776451 0.07464401 0.06720632 0.04898502\n",
            " 0.0428154  0.02714032]\n",
            "LDA Explained Variance Ratio: [0.71429654 0.28570346]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 9 ###\n",
            "\n",
            "Naïve Search Selected Features: [ 6  3  9 10  0  1  4  5  8]\n",
            "Step-wise Forward Selection Selected Features: [ 0  1  2  3  4  6  9 10 12]\n",
            "Step-wise Backward Removal Selected Features: [ 0  2  3  6  7  8  9 10 12]\n",
            "Bidirectional Search Selected Features: [0, 1, 2, 3, 4, 6, 9, 10, 12]\n",
            "Step-wise Floating Forward Selection Selected Features: [0, 2, 3, 4, 5, 6, 9, 10, 12]\n",
            "Step-wise Floating Backward Removal Selected Features: [0, 1, 2, 3, 6, 7, 9, 11, 12]\n",
            "PCA Explained Variance Ratio: [0.3587757  0.18347789 0.11776451 0.07464401 0.06720632 0.04898502\n",
            " 0.0428154  0.02714032 0.02259301]\n",
            "LDA Explained Variance Ratio: [0.71429654 0.28570346]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 10 ###\n",
            "\n",
            "Naïve Search Selected Features: [10  6 12  1  2  3  4  8  5  0]\n",
            "Step-wise Forward Selection Selected Features: [ 0  1  2  3  4  6  9 10 11 12]\n",
            "Step-wise Backward Removal Selected Features: [ 0  2  3  6  7  8  9 10 11 12]\n",
            "Bidirectional Search Selected Features: [0, 1, 2, 3, 4, 6, 9, 10, 11, 12]\n",
            "Step-wise Floating Forward Selection Selected Features: [0, 2, 3, 4, 5, 6, 8, 9, 10, 12]\n",
            "Step-wise Floating Backward Removal Selected Features: [0, 1, 2, 3, 4, 6, 7, 9, 11, 12]\n",
            "PCA Explained Variance Ratio: [0.3587757  0.18347789 0.11776451 0.07464401 0.06720632 0.04898502\n",
            " 0.0428154  0.02714032 0.02259301 0.01902481]\n",
            "LDA Explained Variance Ratio: [0.71429654 0.28570346]\n",
            "Evaluated Decision Tree with Naive_1: 0.5000 accuracy\n",
            "Evaluated KNN with Naive_1: 0.5833 accuracy\n",
            "Evaluated Naive Bayes with Naive_1: 0.5000 accuracy\n",
            "Evaluated SVM with Naive_1: 0.6111 accuracy\n",
            "Evaluated ANN with Naive_1: 0.5833 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_1: 0.6944 accuracy\n",
            "Evaluated KNN with SFS_Forward_1: 0.7500 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_1: 0.8333 accuracy\n",
            "Evaluated SVM with SFS_Forward_1: 0.8056 accuracy\n",
            "Evaluated ANN with SFS_Forward_1: 0.8333 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_1: 0.5278 accuracy\n",
            "Evaluated KNN with SFS_Backward_1: 0.6667 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_1: 0.8056 accuracy\n",
            "Evaluated SVM with SFS_Backward_1: 0.7778 accuracy\n",
            "Evaluated ANN with SFS_Backward_1: 0.8333 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_1: 0.6944 accuracy\n",
            "Evaluated KNN with SFS_Bi_1: 0.7500 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_1: 0.8333 accuracy\n",
            "Evaluated SVM with SFS_Bi_1: 0.8056 accuracy\n",
            "Evaluated ANN with SFS_Bi_1: 0.8056 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_1: 0.6944 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_1: 0.7500 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_1: 0.8333 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_1: 0.8056 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_1: 0.8056 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_1: 0.6944 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_1: 0.7500 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_1: 0.8333 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_1: 0.8056 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_1: 0.8333 accuracy\n",
            "Evaluated Decision Tree with PCA_1: 0.8056 accuracy\n",
            "Evaluated KNN with PCA_1: 0.8333 accuracy\n",
            "Evaluated Naive Bayes with PCA_1: 0.8611 accuracy\n",
            "Evaluated SVM with PCA_1: 0.8611 accuracy\n",
            "Evaluated ANN with PCA_1: 0.8611 accuracy\n",
            "Evaluated Decision Tree with LDA_1: 0.8889 accuracy\n",
            "Evaluated KNN with LDA_1: 0.8889 accuracy\n",
            "Evaluated Naive Bayes with LDA_1: 0.8889 accuracy\n",
            "Evaluated SVM with LDA_1: 0.8889 accuracy\n",
            "Evaluated ANN with LDA_1: 0.8889 accuracy\n",
            "Evaluated Decision Tree with Naive_2: 0.8889 accuracy\n",
            "Evaluated KNN with Naive_2: 0.8056 accuracy\n",
            "Evaluated Naive Bayes with Naive_2: 0.8056 accuracy\n",
            "Evaluated SVM with Naive_2: 0.7500 accuracy\n",
            "Evaluated ANN with Naive_2: 0.7500 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_2: 0.8889 accuracy\n",
            "Evaluated KNN with SFS_Forward_2: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_2: 0.9167 accuracy\n",
            "Evaluated SVM with SFS_Forward_2: 0.9167 accuracy\n",
            "Evaluated ANN with SFS_Forward_2: 0.8889 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_2: 0.8333 accuracy\n",
            "Evaluated KNN with SFS_Backward_2: 0.9167 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_2: 0.9444 accuracy\n",
            "Evaluated SVM with SFS_Backward_2: 0.9444 accuracy\n",
            "Evaluated ANN with SFS_Backward_2: 0.9444 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_2: 0.8889 accuracy\n",
            "Evaluated KNN with SFS_Bi_2: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_2: 0.9167 accuracy\n",
            "Evaluated SVM with SFS_Bi_2: 0.9167 accuracy\n",
            "Evaluated ANN with SFS_Bi_2: 0.8889 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_2: 0.8889 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_2: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_2: 0.9167 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_2: 0.9167 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_2: 0.8889 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_2: 0.9167 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_2: 0.9167 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_2: 0.9167 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_2: 0.9167 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_2: 0.9444 accuracy\n",
            "Evaluated Decision Tree with PCA_2: 1.0000 accuracy\n",
            "Evaluated KNN with PCA_2: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with PCA_2: 0.9722 accuracy\n",
            "Evaluated SVM with PCA_2: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_2: 1.0000 accuracy\n",
            "Evaluated Decision Tree with LDA_2: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_2: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_2: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_2: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_2: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_3: 0.8333 accuracy\n",
            "Evaluated KNN with Naive_3: 0.9167 accuracy\n",
            "Evaluated Naive Bayes with Naive_3: 0.9167 accuracy\n",
            "Evaluated SVM with Naive_3: 0.9167 accuracy\n",
            "Evaluated ANN with Naive_3: 0.8889 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_3: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Forward_3: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_3: 0.9444 accuracy\n",
            "Evaluated SVM with SFS_Forward_3: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Forward_3: 0.9444 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_3: 1.0000 accuracy\n",
            "Evaluated KNN with SFS_Backward_3: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_3: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Backward_3: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Backward_3: 0.9722 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_3: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Bi_3: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_3: 0.9444 accuracy\n",
            "Evaluated SVM with SFS_Bi_3: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Bi_3: 0.9444 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_3: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_3: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_3: 0.9444 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_3: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_3: 0.9444 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_3: 0.9167 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_3: 0.9167 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_3: 0.9444 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_3: 0.9444 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_3: 0.9722 accuracy\n",
            "Evaluated Decision Tree with PCA_3: 1.0000 accuracy\n",
            "Evaluated KNN with PCA_3: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with PCA_3: 1.0000 accuracy\n",
            "Evaluated SVM with PCA_3: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_3: 0.9722 accuracy\n",
            "Evaluated Decision Tree with LDA_3: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_3: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_3: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_3: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_3: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_4: 0.8333 accuracy\n",
            "Evaluated KNN with Naive_4: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with Naive_4: 0.8056 accuracy\n",
            "Evaluated SVM with Naive_4: 0.8889 accuracy\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluated ANN with Naive_4: 0.8889 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_4: 0.9722 accuracy\n",
            "Evaluated KNN with SFS_Forward_4: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_4: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Forward_4: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Forward_4: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_4: 1.0000 accuracy\n",
            "Evaluated KNN with SFS_Backward_4: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_4: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Backward_4: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Backward_4: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_4: 0.9722 accuracy\n",
            "Evaluated KNN with SFS_Bi_4: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_4: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Bi_4: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Bi_4: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_4: 0.9722 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_4: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_4: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_4: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_4: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_4: 0.9167 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_4: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_4: 0.9444 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_4: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_4: 1.0000 accuracy\n",
            "Evaluated Decision Tree with PCA_4: 1.0000 accuracy\n",
            "Evaluated KNN with PCA_4: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with PCA_4: 0.9722 accuracy\n",
            "Evaluated SVM with PCA_4: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_4: 1.0000 accuracy\n",
            "Evaluated Decision Tree with LDA_4: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_4: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_4: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_4: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_4: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_5: 0.9167 accuracy\n",
            "Evaluated KNN with Naive_5: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with Naive_5: 0.9444 accuracy\n",
            "Evaluated SVM with Naive_5: 0.9722 accuracy\n",
            "Evaluated ANN with Naive_5: 0.9722 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_5: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Forward_5: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_5: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Forward_5: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Forward_5: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_5: 1.0000 accuracy\n",
            "Evaluated KNN with SFS_Backward_5: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_5: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Backward_5: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Backward_5: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_5: 0.9722 accuracy\n",
            "Evaluated KNN with SFS_Bi_5: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_5: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Bi_5: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Bi_5: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_5: 0.9722 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_5: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_5: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_5: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_5: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_5: 1.0000 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_5: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_5: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_5: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_5: 1.0000 accuracy\n",
            "Evaluated Decision Tree with PCA_5: 0.9722 accuracy\n",
            "Evaluated KNN with PCA_5: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with PCA_5: 1.0000 accuracy\n",
            "Evaluated SVM with PCA_5: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_5: 1.0000 accuracy\n",
            "Evaluated Decision Tree with LDA_5: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_5: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_5: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_5: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_5: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_6: 0.9167 accuracy\n",
            "Evaluated KNN with Naive_6: 0.9167 accuracy\n",
            "Evaluated Naive Bayes with Naive_6: 1.0000 accuracy\n",
            "Evaluated SVM with Naive_6: 0.9444 accuracy\n",
            "Evaluated ANN with Naive_6: 0.9722 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_6: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Forward_6: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_6: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Forward_6: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Forward_6: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_6: 1.0000 accuracy\n",
            "Evaluated KNN with SFS_Backward_6: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_6: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Backward_6: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Backward_6: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_6: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Bi_6: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_6: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Bi_6: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Bi_6: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_6: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_6: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_6: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_6: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_6: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_6: 1.0000 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_6: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_6: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_6: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_6: 0.9722 accuracy\n",
            "Evaluated Decision Tree with PCA_6: 1.0000 accuracy\n",
            "Evaluated KNN with PCA_6: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with PCA_6: 1.0000 accuracy\n",
            "Evaluated SVM with PCA_6: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_6: 1.0000 accuracy\n",
            "Evaluated Decision Tree with LDA_6: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_6: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_6: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_6: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_6: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_7: 0.9167 accuracy\n",
            "Evaluated KNN with Naive_7: 0.9167 accuracy\n",
            "Evaluated Naive Bayes with Naive_7: 0.8889 accuracy\n",
            "Evaluated SVM with Naive_7: 0.9444 accuracy\n",
            "Evaluated ANN with Naive_7: 0.9167 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_7: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Forward_7: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_7: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Forward_7: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Forward_7: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_7: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Backward_7: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_7: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Backward_7: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Backward_7: 0.9722 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_7: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Bi_7: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_7: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Bi_7: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Bi_7: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_7: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_7: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_7: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_7: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_7: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_7: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_7: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_7: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_7: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_7: 1.0000 accuracy\n",
            "Evaluated Decision Tree with PCA_7: 1.0000 accuracy\n",
            "Evaluated KNN with PCA_7: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with PCA_7: 1.0000 accuracy\n",
            "Evaluated SVM with PCA_7: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_7: 1.0000 accuracy\n",
            "Evaluated Decision Tree with LDA_7: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_7: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_7: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_7: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_7: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_8: 0.9444 accuracy\n",
            "Evaluated KNN with Naive_8: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with Naive_8: 0.9722 accuracy\n",
            "Evaluated SVM with Naive_8: 1.0000 accuracy\n",
            "Evaluated ANN with Naive_8: 0.9722 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_8: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Forward_8: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_8: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Forward_8: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Forward_8: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_8: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Backward_8: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_8: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Backward_8: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Backward_8: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_8: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Bi_8: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_8: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Bi_8: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Bi_8: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_8: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_8: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_8: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_8: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_8: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_8: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_8: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_8: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_8: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_8: 1.0000 accuracy\n",
            "Evaluated Decision Tree with PCA_8: 0.9722 accuracy\n",
            "Evaluated KNN with PCA_8: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with PCA_8: 1.0000 accuracy\n",
            "Evaluated SVM with PCA_8: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_8: 0.9722 accuracy\n",
            "Evaluated Decision Tree with LDA_8: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_8: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_8: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_8: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_8: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_9: 0.9444 accuracy\n",
            "Evaluated KNN with Naive_9: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with Naive_9: 0.9167 accuracy\n",
            "Evaluated SVM with Naive_9: 0.9444 accuracy\n",
            "Evaluated ANN with Naive_9: 0.9444 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_9: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Forward_9: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_9: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Forward_9: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Forward_9: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_9: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Backward_9: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_9: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Backward_9: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Backward_9: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_9: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Bi_9: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_9: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Bi_9: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Bi_9: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_9: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_9: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_9: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_9: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_9: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_9: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_9: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_9: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_9: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_9: 1.0000 accuracy\n",
            "Evaluated Decision Tree with PCA_9: 1.0000 accuracy\n",
            "Evaluated KNN with PCA_9: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with PCA_9: 1.0000 accuracy\n",
            "Evaluated SVM with PCA_9: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_9: 1.0000 accuracy\n",
            "Evaluated Decision Tree with LDA_9: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_9: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_9: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_9: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_9: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_10: 0.9444 accuracy\n",
            "Evaluated KNN with Naive_10: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with Naive_10: 1.0000 accuracy\n",
            "Evaluated SVM with Naive_10: 0.9722 accuracy\n",
            "Evaluated ANN with Naive_10: 0.9722 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_10: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Forward_10: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_10: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Forward_10: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Forward_10: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_10: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Backward_10: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_10: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Backward_10: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Backward_10: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_10: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Bi_10: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_10: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Bi_10: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Bi_10: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_10: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_10: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_10: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_10: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_10: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_10: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_10: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_10: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_10: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_10: 1.0000 accuracy\n",
            "Evaluated Decision Tree with PCA_10: 0.9722 accuracy\n",
            "Evaluated KNN with PCA_10: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with PCA_10: 1.0000 accuracy\n",
            "Evaluated SVM with PCA_10: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_10: 0.9722 accuracy\n",
            "Evaluated Decision Tree with LDA_10: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_10: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_10: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_10: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_10: 1.0000 accuracy\n",
            "Feature selection and dimensionality reduction completed.\n",
            "Results saved to 'wine_dataset_metrics.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch wine dataset\n",
        "wine = fetch_ucirepo(id=109)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = wine.data.features\n",
        "y = wine.data.targets\n",
        "\n",
        "# metadata\n",
        "print(wine.metadata)\n",
        "\n",
        "# variable information\n",
        "print(wine.variables)\n",
        "\n",
        "# Create a copy of X and y to avoid the SettingWithCopyWarning\n",
        "X_copy = X.copy()\n",
        "y_copy = y.copy()\n",
        "\n",
        "# Check for missing values before preprocessing\n",
        "print(f\"Missing values in original X: {X_copy.isnull().sum().sum()}\")\n",
        "print(f\"Missing values in original y: {y_copy.isnull().sum().sum()}\")\n",
        "\n",
        "# Handle missing values properly\n",
        "X_copy = X_copy.replace('?', np.nan)\n",
        "X_copy = X_copy.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill remaining missing values with the mean of each column\n",
        "for col in X_copy.columns:\n",
        "    if X_copy[col].isnull().sum() > 0:\n",
        "        X_copy[col] = X_copy[col].fillna(X_copy[col].mean())\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(f\"Missing values after preprocessing: {X_copy.isnull().sum().sum()}\")\n",
        "\n",
        "# For the wine dataset, we'll use the class attribute as our target\n",
        "# The target is already properly formatted in the UCI repo\n",
        "target = y_copy.iloc[:, 0]\n",
        "\n",
        "# Make sure target has no missing values\n",
        "target = target.fillna(target.mode()[0])  # Fill with mode since this is a classification task\n",
        "\n",
        "# Standardizing features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_copy)\n",
        "\n",
        "# Double-check for NaN values after scaling\n",
        "if np.isnan(X_scaled).any():\n",
        "    print(\"Warning: NaN values found after scaling. Replacing with 0...\")\n",
        "    X_scaled = np.nan_to_num(X_scaled)\n",
        "\n",
        "# Splitting data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression as the base classifier\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Function to evaluate and collect performance metrics\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"f1_score\": f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    }\n",
        "\n",
        "# Classifiers to evaluate\n",
        "classifiers = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"ANN\": MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "# Dimensionality reduction methods\n",
        "methods = {}\n",
        "# Test a range of dimensions\n",
        "for d in range(1, min(11, X_copy.shape[1])):  # Up to 10 features or max available\n",
        "    print(f\"\\n### Feature Selection and Dimensionality Reduction with d = {d} ###\\n\")\n",
        "\n",
        "    # Naïve Search (Random selection of d features)\n",
        "    feature_indices = np.random.choice(range(X_scaled.shape[1]), d, replace=False)\n",
        "    X_train_naive = X_train[:, feature_indices]\n",
        "    X_test_naive = X_test[:, feature_indices]\n",
        "    print(f\"Naïve Search Selected Features: {feature_indices}\")\n",
        "    methods[f\"Naive_{d}\"] = (X_train_naive, X_test_naive)\n",
        "\n",
        "    try:\n",
        "        # Step-wise Forward Selection\n",
        "        sfs_forward = SequentialFeatureSelector(model, n_features_to_select=d, direction='forward').fit(X_train, y_train)\n",
        "        X_train_sfs = sfs_forward.transform(X_train)\n",
        "        X_test_sfs = sfs_forward.transform(X_test)\n",
        "        print(f\"Step-wise Forward Selection Selected Features: {np.where(sfs_forward.get_support())[0]}\")\n",
        "        methods[f\"SFS_Forward_{d}\"] = (X_train_sfs, X_test_sfs)\n",
        "    except Exception as e:\n",
        "        print(f\"Step-wise Forward Selection failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        # Step-wise Backward Removal\n",
        "        sfs_backward = SequentialFeatureSelector(model, n_features_to_select=d, direction='backward').fit(X_train, y_train)\n",
        "        X_train_sbs = sfs_backward.transform(X_train)\n",
        "        X_test_sbs = sfs_backward.transform(X_test)\n",
        "        print(f\"Step-wise Backward Removal Selected Features: {np.where(sfs_backward.get_support())[0]}\")\n",
        "        methods[f\"SFS_Backward_{d}\"] = (X_train_sbs, X_test_sbs)\n",
        "    except Exception as e:\n",
        "        print(f\"Step-wise Backward Removal failed: {e}\")\n",
        "\n",
        "    # Bidirectional Search (using MLxtend's SFS)\n",
        "    try:\n",
        "        sfs_bi = SFS(model, k_features=d, forward=True, floating=False, scoring='accuracy', cv=5).fit(X_train, y_train)\n",
        "        X_train_bi = X_train[:, list(sfs_bi.k_feature_idx_)]\n",
        "        X_test_bi = X_test[:, list(sfs_bi.k_feature_idx_)]\n",
        "        print(f\"Bidirectional Search Selected Features: {list(sfs_bi.k_feature_idx_)}\")\n",
        "        methods[f\"SFS_Bi_{d}\"] = (X_train_bi, X_test_bi)\n",
        "    except Exception as e:\n",
        "        print(f\"Bidirectional Search failed: {e}\")\n",
        "\n",
        "    # Step-wise Floating Forward Selection\n",
        "    try:\n",
        "        sfs_float_forward = SFS(model, k_features=d, forward=True, floating=True, scoring='accuracy', cv=5).fit(X_train, y_train)\n",
        "        X_train_sfsf = X_train[:, list(sfs_float_forward.k_feature_idx_)]\n",
        "        X_test_sfsf = X_test[:, list(sfs_float_forward.k_feature_idx_)]\n",
        "        print(f\"Step-wise Floating Forward Selection Selected Features: {list(sfs_float_forward.k_feature_idx_)}\")\n",
        "        methods[f\"SFS_Float_Forward_{d}\"] = (X_train_sfsf, X_test_sfsf)\n",
        "    except Exception as e:\n",
        "        print(f\"Floating Forward Selection failed: {e}\")\n",
        "\n",
        "    # Step-wise Floating Backward Removal\n",
        "    try:\n",
        "        sfs_float_backward = SFS(model, k_features=d, forward=False, floating=True, scoring='accuracy', cv=5).fit(X_train, y_train)\n",
        "        X_train_sfsb = X_train[:, list(sfs_float_backward.k_feature_idx_)]\n",
        "        X_test_sfsb = X_test[:, list(sfs_float_backward.k_feature_idx_)]\n",
        "        print(f\"Step-wise Floating Backward Removal Selected Features: {list(sfs_float_backward.k_feature_idx_)}\")\n",
        "        methods[f\"SFS_Float_Backward_{d}\"] = (X_train_sfsb, X_test_sfsb)\n",
        "    except Exception as e:\n",
        "        print(f\"Floating Backward Selection failed: {e}\")\n",
        "\n",
        "    # Principal Component Analysis (PCA)\n",
        "    try:\n",
        "        pca = PCA(n_components=d)\n",
        "        X_train_pca = pca.fit_transform(X_train)\n",
        "        X_test_pca = pca.transform(X_test)\n",
        "        print(f\"PCA Explained Variance Ratio: {pca.explained_variance_ratio_}\")\n",
        "        methods[f\"PCA_{d}\"] = (X_train_pca, X_test_pca)\n",
        "    except Exception as e:\n",
        "        print(f\"PCA failed: {e}\")\n",
        "\n",
        "    # Linear Discriminant Analysis (LDA)\n",
        "    try:\n",
        "        n_components = min(d, len(np.unique(y_train)) - 1)  # LDA components <= classes - 1\n",
        "        if n_components > 0:  # Only perform LDA if we have enough classes\n",
        "            lda = LDA(n_components=n_components)\n",
        "            X_train_lda = lda.fit_transform(X_train, y_train)\n",
        "            X_test_lda = lda.transform(X_test)\n",
        "            if hasattr(lda, 'explained_variance_ratio_'):\n",
        "                print(f\"LDA Explained Variance Ratio: {lda.explained_variance_ratio_}\")\n",
        "            methods[f\"LDA_{d}\"] = (X_train_lda, X_test_lda)\n",
        "    except Exception as e:\n",
        "        print(f\"LDA failed: {e}\")\n",
        "\n",
        "# Evaluate classifiers on each method\n",
        "results = {}\n",
        "for method_name, (X_tr, X_te) in methods.items():\n",
        "    results[method_name] = {}\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        try:\n",
        "            results[method_name][clf_name] = evaluate_model(clf, X_tr, X_te, y_train, y_test)\n",
        "            print(f\"Evaluated {clf_name} with {method_name}: {results[method_name][clf_name]['accuracy']:.4f} accuracy\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating {clf_name} with {method_name}: {e}\")\n",
        "            results[method_name][clf_name] = {\"accuracy\": 0, \"precision\": 0, \"recall\": 0, \"f1_score\": 0}\n",
        "\n",
        "# Convert results to DataFrame for analysis\n",
        "results_df = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
        "                                   for i in results.keys()\n",
        "                                   for j in results[i].keys()},\n",
        "                                   orient='index')\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results_df.to_csv(\"wine_dataset_metrics.csv\")\n",
        "print(\"Feature selection and dimensionality reduction completed.\")\n",
        "print(\"Results saved to 'wine_dataset_metrics.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3dAKFNk4_A7",
        "outputId": "665708c2-be57-4bf7-b297-b561477b353e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n"
          ]
        }
      ],
      "source": [
        "pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLcy9ibpLn0n",
        "outputId": "f7c999fc-5e84-4144-e2ea-6c9edad3c1a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluated Decision Tree with Naive_1: 0.5000 accuracy\n",
            "Evaluated KNN with Naive_1: 0.5833 accuracy\n",
            "Evaluated Naive Bayes with Naive_1: 0.5000 accuracy\n",
            "Evaluated SVM with Naive_1: 0.6111 accuracy\n",
            "Evaluated ANN with Naive_1: 0.6111 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_1: 0.6944 accuracy\n",
            "Evaluated KNN with SFS_Forward_1: 0.7500 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_1: 0.8333 accuracy\n",
            "Evaluated SVM with SFS_Forward_1: 0.8056 accuracy\n",
            "Evaluated ANN with SFS_Forward_1: 0.8333 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_1: 0.5278 accuracy\n",
            "Evaluated KNN with SFS_Backward_1: 0.6667 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_1: 0.8056 accuracy\n",
            "Evaluated SVM with SFS_Backward_1: 0.7778 accuracy\n",
            "Evaluated ANN with SFS_Backward_1: 0.8056 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_1: 0.6944 accuracy\n",
            "Evaluated KNN with SFS_Bi_1: 0.7500 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_1: 0.8333 accuracy\n",
            "Evaluated SVM with SFS_Bi_1: 0.8056 accuracy\n",
            "Evaluated ANN with SFS_Bi_1: 0.8333 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_1: 0.6944 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_1: 0.7500 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_1: 0.8333 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_1: 0.8056 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_1: 0.8333 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_1: 0.6944 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_1: 0.7500 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_1: 0.8333 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_1: 0.8056 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_1: 0.8333 accuracy\n",
            "Evaluated Decision Tree with PCA_1: 0.8056 accuracy\n",
            "Evaluated KNN with PCA_1: 0.8333 accuracy\n",
            "Evaluated Naive Bayes with PCA_1: 0.8611 accuracy\n",
            "Evaluated SVM with PCA_1: 0.8611 accuracy\n",
            "Evaluated ANN with PCA_1: 0.8611 accuracy\n",
            "Evaluated Decision Tree with LDA_1: 0.8889 accuracy\n",
            "Evaluated KNN with LDA_1: 0.8889 accuracy\n",
            "Evaluated Naive Bayes with LDA_1: 0.8889 accuracy\n",
            "Evaluated SVM with LDA_1: 0.8889 accuracy\n",
            "Evaluated ANN with LDA_1: 0.8889 accuracy\n",
            "Evaluated Decision Tree with Naive_2: 0.8889 accuracy\n",
            "Evaluated KNN with Naive_2: 0.8056 accuracy\n",
            "Evaluated Naive Bayes with Naive_2: 0.8056 accuracy\n",
            "Evaluated SVM with Naive_2: 0.7500 accuracy\n",
            "Evaluated ANN with Naive_2: 0.7500 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_2: 0.8889 accuracy\n",
            "Evaluated KNN with SFS_Forward_2: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_2: 0.9167 accuracy\n",
            "Evaluated SVM with SFS_Forward_2: 0.9167 accuracy\n",
            "Evaluated ANN with SFS_Forward_2: 0.8889 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_2: 0.8333 accuracy\n",
            "Evaluated KNN with SFS_Backward_2: 0.9167 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_2: 0.9444 accuracy\n",
            "Evaluated SVM with SFS_Backward_2: 0.9444 accuracy\n",
            "Evaluated ANN with SFS_Backward_2: 0.9444 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_2: 0.8889 accuracy\n",
            "Evaluated KNN with SFS_Bi_2: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_2: 0.9167 accuracy\n",
            "Evaluated SVM with SFS_Bi_2: 0.9167 accuracy\n",
            "Evaluated ANN with SFS_Bi_2: 0.8889 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_2: 0.8889 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_2: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_2: 0.9167 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_2: 0.9167 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_2: 0.8889 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_2: 0.9167 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_2: 0.9167 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_2: 0.9167 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_2: 0.9167 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_2: 0.9444 accuracy\n",
            "Evaluated Decision Tree with PCA_2: 1.0000 accuracy\n",
            "Evaluated KNN with PCA_2: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with PCA_2: 0.9722 accuracy\n",
            "Evaluated SVM with PCA_2: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_2: 1.0000 accuracy\n",
            "Evaluated Decision Tree with LDA_2: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_2: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_2: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_2: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_2: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_3: 0.8056 accuracy\n",
            "Evaluated KNN with Naive_3: 0.9167 accuracy\n",
            "Evaluated Naive Bayes with Naive_3: 0.9167 accuracy\n",
            "Evaluated SVM with Naive_3: 0.9167 accuracy\n",
            "Evaluated ANN with Naive_3: 0.8889 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_3: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Forward_3: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_3: 0.9444 accuracy\n",
            "Evaluated SVM with SFS_Forward_3: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Forward_3: 0.9444 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_3: 1.0000 accuracy\n",
            "Evaluated KNN with SFS_Backward_3: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_3: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Backward_3: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Backward_3: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_3: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Bi_3: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_3: 0.9444 accuracy\n",
            "Evaluated SVM with SFS_Bi_3: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Bi_3: 0.9444 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_3: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_3: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_3: 0.9444 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_3: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_3: 0.9444 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_3: 0.9167 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_3: 0.9167 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_3: 0.9444 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_3: 0.9444 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_3: 1.0000 accuracy\n",
            "Evaluated Decision Tree with PCA_3: 0.9722 accuracy\n",
            "Evaluated KNN with PCA_3: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with PCA_3: 1.0000 accuracy\n",
            "Evaluated SVM with PCA_3: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_3: 0.9722 accuracy\n",
            "Evaluated Decision Tree with LDA_3: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_3: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_3: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_3: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_3: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_4: 0.8333 accuracy\n",
            "Evaluated KNN with Naive_4: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with Naive_4: 0.8056 accuracy\n",
            "Evaluated SVM with Naive_4: 0.8889 accuracy\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluated ANN with Naive_4: 0.8889 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_4: 0.9722 accuracy\n",
            "Evaluated KNN with SFS_Forward_4: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_4: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Forward_4: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Forward_4: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_4: 1.0000 accuracy\n",
            "Evaluated KNN with SFS_Backward_4: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_4: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Backward_4: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Backward_4: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_4: 0.9722 accuracy\n",
            "Evaluated KNN with SFS_Bi_4: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_4: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Bi_4: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Bi_4: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_4: 0.9722 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_4: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_4: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_4: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_4: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_4: 0.9167 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_4: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_4: 0.9444 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_4: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_4: 0.9722 accuracy\n",
            "Evaluated Decision Tree with PCA_4: 1.0000 accuracy\n",
            "Evaluated KNN with PCA_4: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with PCA_4: 0.9722 accuracy\n",
            "Evaluated SVM with PCA_4: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_4: 1.0000 accuracy\n",
            "Evaluated Decision Tree with LDA_4: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_4: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_4: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_4: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_4: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_5: 0.9167 accuracy\n",
            "Evaluated KNN with Naive_5: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with Naive_5: 0.9444 accuracy\n",
            "Evaluated SVM with Naive_5: 0.9722 accuracy\n",
            "Evaluated ANN with Naive_5: 0.9722 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_5: 0.9722 accuracy\n",
            "Evaluated KNN with SFS_Forward_5: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_5: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Forward_5: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Forward_5: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_5: 1.0000 accuracy\n",
            "Evaluated KNN with SFS_Backward_5: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_5: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Backward_5: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Backward_5: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_5: 0.9722 accuracy\n",
            "Evaluated KNN with SFS_Bi_5: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_5: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Bi_5: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Bi_5: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_5: 0.9722 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_5: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_5: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_5: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_5: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_5: 1.0000 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_5: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_5: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_5: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_5: 1.0000 accuracy\n",
            "Evaluated Decision Tree with PCA_5: 1.0000 accuracy\n",
            "Evaluated KNN with PCA_5: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with PCA_5: 1.0000 accuracy\n",
            "Evaluated SVM with PCA_5: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_5: 1.0000 accuracy\n",
            "Evaluated Decision Tree with LDA_5: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_5: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_5: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_5: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_5: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_6: 0.8611 accuracy\n",
            "Evaluated KNN with Naive_6: 0.9167 accuracy\n",
            "Evaluated Naive Bayes with Naive_6: 1.0000 accuracy\n",
            "Evaluated SVM with Naive_6: 0.9444 accuracy\n",
            "Evaluated ANN with Naive_6: 0.9722 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_6: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Forward_6: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_6: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Forward_6: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Forward_6: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_6: 0.9722 accuracy\n",
            "Evaluated KNN with SFS_Backward_6: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_6: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Backward_6: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Backward_6: 0.9722 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_6: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Bi_6: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_6: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Bi_6: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Bi_6: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_6: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_6: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_6: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_6: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_6: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_6: 1.0000 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_6: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_6: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_6: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_6: 0.9722 accuracy\n",
            "Evaluated Decision Tree with PCA_6: 1.0000 accuracy\n",
            "Evaluated KNN with PCA_6: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with PCA_6: 1.0000 accuracy\n",
            "Evaluated SVM with PCA_6: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_6: 1.0000 accuracy\n",
            "Evaluated Decision Tree with LDA_6: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_6: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_6: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_6: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_6: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_7: 0.9167 accuracy\n",
            "Evaluated KNN with Naive_7: 0.9167 accuracy\n",
            "Evaluated Naive Bayes with Naive_7: 0.8889 accuracy\n",
            "Evaluated SVM with Naive_7: 0.9444 accuracy\n",
            "Evaluated ANN with Naive_7: 0.9444 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_7: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Forward_7: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_7: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Forward_7: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Forward_7: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_7: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Backward_7: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_7: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Backward_7: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Backward_7: 0.9722 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_7: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Bi_7: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_7: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Bi_7: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Bi_7: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_7: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_7: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_7: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_7: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_7: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_7: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_7: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_7: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_7: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_7: 1.0000 accuracy\n",
            "Evaluated Decision Tree with PCA_7: 1.0000 accuracy\n",
            "Evaluated KNN with PCA_7: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with PCA_7: 1.0000 accuracy\n",
            "Evaluated SVM with PCA_7: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_7: 1.0000 accuracy\n",
            "Evaluated Decision Tree with LDA_7: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_7: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_7: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_7: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_7: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_8: 0.9444 accuracy\n",
            "Evaluated KNN with Naive_8: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with Naive_8: 0.9722 accuracy\n",
            "Evaluated SVM with Naive_8: 1.0000 accuracy\n",
            "Evaluated ANN with Naive_8: 0.9722 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_8: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Forward_8: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_8: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Forward_8: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Forward_8: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_8: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Backward_8: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_8: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Backward_8: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Backward_8: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_8: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Bi_8: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_8: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Bi_8: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Bi_8: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_8: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_8: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_8: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_8: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_8: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_8: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_8: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_8: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_8: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_8: 1.0000 accuracy\n",
            "Evaluated Decision Tree with PCA_8: 0.9722 accuracy\n",
            "Evaluated KNN with PCA_8: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with PCA_8: 1.0000 accuracy\n",
            "Evaluated SVM with PCA_8: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_8: 0.9722 accuracy\n",
            "Evaluated Decision Tree with LDA_8: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_8: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_8: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_8: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_8: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_9: 0.9444 accuracy\n",
            "Evaluated KNN with Naive_9: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with Naive_9: 0.9167 accuracy\n",
            "Evaluated SVM with Naive_9: 0.9444 accuracy\n",
            "Evaluated ANN with Naive_9: 0.9722 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_9: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Forward_9: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_9: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Forward_9: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Forward_9: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_9: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Backward_9: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_9: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Backward_9: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Backward_9: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_9: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Bi_9: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_9: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Bi_9: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Bi_9: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_9: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_9: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_9: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_9: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_9: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_9: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_9: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_9: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_9: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_9: 1.0000 accuracy\n",
            "Evaluated Decision Tree with PCA_9: 1.0000 accuracy\n",
            "Evaluated KNN with PCA_9: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with PCA_9: 1.0000 accuracy\n",
            "Evaluated SVM with PCA_9: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_9: 0.9722 accuracy\n",
            "Evaluated Decision Tree with LDA_9: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_9: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_9: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_9: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_9: 1.0000 accuracy\n",
            "Evaluated Decision Tree with Naive_10: 0.9444 accuracy\n",
            "Evaluated KNN with Naive_10: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with Naive_10: 1.0000 accuracy\n",
            "Evaluated SVM with Naive_10: 0.9722 accuracy\n",
            "Evaluated ANN with Naive_10: 0.9722 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_10: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Forward_10: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_10: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Forward_10: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Forward_10: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Backward_10: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Backward_10: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Backward_10: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Backward_10: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Backward_10: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Bi_10: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Bi_10: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Bi_10: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Bi_10: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Bi_10: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Forward_10: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Forward_10: 0.9722 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Forward_10: 1.0000 accuracy\n",
            "Evaluated SVM with SFS_Float_Forward_10: 0.9722 accuracy\n",
            "Evaluated ANN with SFS_Float_Forward_10: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Float_Backward_10: 0.9444 accuracy\n",
            "Evaluated KNN with SFS_Float_Backward_10: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Float_Backward_10: 0.9722 accuracy\n",
            "Evaluated SVM with SFS_Float_Backward_10: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Float_Backward_10: 1.0000 accuracy\n",
            "Evaluated Decision Tree with PCA_10: 1.0000 accuracy\n",
            "Evaluated KNN with PCA_10: 0.9444 accuracy\n",
            "Evaluated Naive Bayes with PCA_10: 1.0000 accuracy\n",
            "Evaluated SVM with PCA_10: 1.0000 accuracy\n",
            "Evaluated ANN with PCA_10: 1.0000 accuracy\n",
            "Evaluated Decision Tree with LDA_10: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_10: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_10: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_10: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_10: 1.0000 accuracy\n",
            "Model evaluation completed. Results saved to 'wine_dataset_metrics.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Function to evaluate a classifier\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"f1_score\": f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    }\n",
        "\n",
        "# Classifiers to evaluate\n",
        "classifiers = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"ANN\": MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "# Ensure `methods` dictionary exists before evaluation\n",
        "if \"methods\" in locals():\n",
        "    results = {}\n",
        "    for method_name, (X_tr, X_te) in methods.items():\n",
        "        results[method_name] = {}\n",
        "        for clf_name, clf in classifiers.items():\n",
        "            try:\n",
        "                results[method_name][clf_name] = evaluate_model(clf, X_tr, X_te, y_train, y_test)\n",
        "                print(f\"Evaluated {clf_name} with {method_name}: {results[method_name][clf_name]['accuracy']:.4f} accuracy\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error evaluating {clf_name} with {method_name}: {e}\")\n",
        "                results[method_name][clf_name] = {\"accuracy\": 0, \"precision\": 0, \"recall\": 0, \"f1_score\": 0}\n",
        "\n",
        "    # Convert results to DataFrame for analysis\n",
        "    results_df = pd.DataFrame.from_dict({(i, j): results[i][j] for i in results.keys() for j in results[i].keys()},\n",
        "                                       orient='index')\n",
        "\n",
        "    # Save the results to a CSV file\n",
        "    results_df.to_csv(\"wine_dataset_metrics.csv\")\n",
        "\n",
        "    print(\"Model evaluation completed. Results saved to 'wine_dataset_metrics.csv'\")\n",
        "else:\n",
        "    print(\"Error: `methods` dictionary not found. Ensure feature selection has been performed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th7GmQEpLy42"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the performance metrics data\n",
        "results_df = pd.read_csv(\"wine_dataset_metrics.csv\", index_col=[0, 1])\n",
        "\n",
        "# Ensure the index is a MultiIndex\n",
        "if not isinstance(results_df.index, pd.MultiIndex):\n",
        "    results_df.index = pd.MultiIndex.from_tuples(results_df.index)\n",
        "\n",
        "# Check the structure of the DataFrame\n",
        "print(\"DataFrame Structure:\")\n",
        "print(results_df.index)\n",
        "print(results_df.columns)\n",
        "\n",
        "# 4i) Table Summary\n",
        "print(\"Table Summary of Performance Metrics:\")\n",
        "print(results_df)\n",
        "\n",
        "# 4ii) Pie Charts\n",
        "def plot_pie_charts(results_df, metric):\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(18, 18))\n",
        "    fig.suptitle(f'Pie Charts for {metric.capitalize()}', fontsize=20)\n",
        "\n",
        "    for idx, (method, data) in enumerate(results_df.groupby(level=0)):\n",
        "        if idx >= 9:\n",
        "            break  # Limit to 9 pie charts to fit subplot grid\n",
        "\n",
        "        ax = axes[idx // 3, idx % 3]\n",
        "        data = data.xs(method, level=0)[metric]  # Extract data using xs\n",
        "        data.plot.pie(ax=ax, autopct='%1.1f%%', title=f'{method}')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_pie_charts(results_df, metric)\n",
        "\n",
        "# 4iii) Bar Charts\n",
        "def plot_bar_charts(results_df, metric):\n",
        "    results_metric = results_df[metric]  # Directly access metric column\n",
        "    results_metric.unstack(level=1).plot(kind='bar', figsize=(15, 10), title=f'Bar Charts for {metric.capitalize()}')\n",
        "\n",
        "    plt.ylabel(metric)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Classifiers', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_bar_charts(results_df, metric)\n",
        "\n",
        "# 4iv) Line Graphs\n",
        "def plot_line_graphs(results_df, metric):\n",
        "    results_metric = results_df[metric]  # Directly access metric column\n",
        "    results_metric.unstack(level=1).plot(kind='line', marker='o', figsize=(15, 10), title=f'Line Graphs for {metric.capitalize()}')\n",
        "\n",
        "    plt.ylabel(metric)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Classifiers', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_line_graphs(results_df, metric)\n",
        "\n",
        "# 4vi) Box Plots\n",
        "def plot_box_plots(results_df, metric):\n",
        "    results_metric = results_df[metric]  # Directly access metric column\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    sns.boxplot(data=results_metric.unstack(level=1))\n",
        "\n",
        "    plt.title(f'Box Plots for {metric.capitalize()}')\n",
        "    plt.ylabel(metric)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_box_plots(results_df, metric)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHoLiuOjMZqp"
      },
      "source": [
        "DATASET 3 : LUNG CANCER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTeuyiG8NjrR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# Fetch lung cancer dataset\n",
        "lung_cancer = fetch_ucirepo(id=62)\n",
        "\n",
        "# Data (as pandas dataframes)\n",
        "X = lung_cancer.data.features\n",
        "y = lung_cancer.data.targets\n",
        "\n",
        "# Display metadata and variable information\n",
        "print(\"Dataset Metadata:\")\n",
        "print(lung_cancer.metadata)\n",
        "print(\"\\nVariable Information:\")\n",
        "print(lung_cancer.variables)\n",
        "\n",
        "# Create a copy of X and y to avoid the SettingWithCopyWarning\n",
        "X_copy = X.copy()\n",
        "y_copy = y.copy()\n",
        "\n",
        "# Check for missing values before preprocessing\n",
        "print(f\"\\nMissing values in original X: {X_copy.isnull().sum().sum()}\")\n",
        "print(f\"Missing values in original y: {y_copy.isnull().sum().sum()}\")\n",
        "\n",
        "# Handle missing values properly\n",
        "X_copy = X_copy.replace('?', np.nan)\n",
        "X_copy = X_copy.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill remaining missing values with the mean of each column\n",
        "for col in X_copy.columns:\n",
        "    if X_copy[col].isnull().sum() > 0:\n",
        "        X_copy[col] = X_copy[col].fillna(X_copy[col].mean())\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(f\"Missing values after preprocessing: {X_copy.isnull().sum().sum()}\")\n",
        "\n",
        "# For the lung cancer dataset, we need to extract the target variable\n",
        "# The target is typically the 'Class' column or similar\n",
        "if 'Class' in y_copy.columns:\n",
        "    target = y_copy['Class']\n",
        "elif 'TARGET' in y_copy.columns:\n",
        "    target = y_copy['TARGET']\n",
        "else:\n",
        "    # If specific target columns don't exist, use the first target column\n",
        "    target_col = y_copy.columns[0]\n",
        "    target = y_copy[target_col]\n",
        "    print(f\"Using {target_col} as target variable\")\n",
        "\n",
        "# Make sure target has no missing values\n",
        "target = target.fillna(target.mode()[0])  # Fill missing values with the most common class\n",
        "\n",
        "# Convert target to numeric if it's not already\n",
        "target = pd.to_numeric(target, errors='coerce')\n",
        "# Check if target needs to be filled again after conversion\n",
        "if target.isnull().sum() > 0:\n",
        "    target = target.fillna(target.mode()[0])\n",
        "\n",
        "# Standardizing features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_copy)\n",
        "\n",
        "# Double-check for NaN values after scaling\n",
        "if np.isnan(X_scaled).any():\n",
        "    print(\"Warning: NaN values found after scaling. Replacing with 0...\")\n",
        "    X_scaled = np.nan_to_num(X_scaled)\n",
        "\n",
        "# Splitting data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression as the base classifier\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Function to evaluate and collect performance metrics\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"f1_score\": f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    }\n",
        "\n",
        "# Classifiers to evaluate\n",
        "classifiers = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"ANN\": MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "# Dimensionality reduction methods\n",
        "methods = {}\n",
        "# Test a range of dimensions\n",
        "for d in range(1, min(11, X_copy.shape[1])):  # Up to 10 features or max available\n",
        "    print(f\"\\n### Feature Selection and Dimensionality Reduction with d = {d} ###\\n\")\n",
        "\n",
        "    # Naïve Search (Random selection of d features)\n",
        "    feature_indices = np.random.choice(range(X_scaled.shape[1]), d, replace=False)\n",
        "    X_train_naive = X_train[:, feature_indices]\n",
        "    X_test_naive = X_test[:, feature_indices]\n",
        "    print(f\"Naïve Search Selected Features: {feature_indices}\")\n",
        "    methods[f\"Naive_{d}\"] = (X_train_naive, X_test_naive)\n",
        "\n",
        "    try:\n",
        "        # Step-wise Forward Selection\n",
        "        sfs_forward = SequentialFeatureSelector(model, n_features_to_select=d, direction='forward').fit(X_train, y_train)\n",
        "        X_train_sfs = sfs_forward.transform(X_train)\n",
        "        X_test_sfs = sfs_forward.transform(X_test)\n",
        "        print(f\"Step-wise Forward Selection Selected Features: {np.where(sfs_forward.get_support())[0]}\")\n",
        "        methods[f\"SFS_Forward_{d}\"] = (X_train_sfs, X_test_sfs)\n",
        "    except Exception as e:\n",
        "        print(f\"Step-wise Forward Selection failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        # Step-wise Backward Removal\n",
        "        sfs_backward = SequentialFeatureSelector(model, n_features_to_select=d, direction='backward').fit(X_train, y_train)\n",
        "        X_train_sbs = sfs_backward.transform(X_train)\n",
        "        X_test_sbs = sfs_backward.transform(X_test)\n",
        "        print(f\"Step-wise Backward Removal Selected Features: {np.where(sfs_backward.get_support())[0]}\")\n",
        "        methods[f\"SFS_Backward_{d}\"] = (X_train_sbs, X_test_sbs)\n",
        "    except Exception as e:\n",
        "        print(f\"Step-wise Backward Removal failed: {e}\")\n",
        "\n",
        "    # Bidirectional Search (using MLxtend's SFS)\n",
        "    try:\n",
        "        sfs_bi = SFS(model, k_features=d, forward=True, floating=False, scoring='accuracy', cv=5).fit(X_train, y_train)\n",
        "        X_train_bi = X_train[:, list(sfs_bi.k_feature_idx_)]\n",
        "        X_test_bi = X_test[:, list(sfs_bi.k_feature_idx_)]\n",
        "        print(f\"Bidirectional Search Selected Features: {list(sfs_bi.k_feature_idx_)}\")\n",
        "        methods[f\"SFS_Bi_{d}\"] = (X_train_bi, X_test_bi)\n",
        "    except Exception as e:\n",
        "        print(f\"Bidirectional Search failed: {e}\")\n",
        "\n",
        "    # Step-wise Floating Forward Selection\n",
        "    try:\n",
        "        sfs_float_forward = SFS(model, k_features=d, forward=True, floating=True, scoring='accuracy', cv=5).fit(X_train, y_train)\n",
        "        X_train_sfsf = X_train[:, list(sfs_float_forward.k_feature_idx_)]\n",
        "        X_test_sfsf = X_test[:, list(sfs_float_forward.k_feature_idx_)]\n",
        "        print(f\"Step-wise Floating Forward Selection Selected Features: {list(sfs_float_forward.k_feature_idx_)}\")\n",
        "        methods[f\"SFS_Float_Forward_{d}\"] = (X_train_sfsf, X_test_sfsf)\n",
        "    except Exception as e:\n",
        "        print(f\"Floating Forward Selection failed: {e}\")\n",
        "\n",
        "    # Step-wise Floating Backward Removal\n",
        "    try:\n",
        "        sfs_float_backward = SFS(model, k_features=d, forward=False, floating=True, scoring='accuracy', cv=5).fit(X_train, y_train)\n",
        "        X_train_sfsb = X_train[:, list(sfs_float_backward.k_feature_idx_)]\n",
        "        X_test_sfsb = X_test[:, list(sfs_float_backward.k_feature_idx_)]\n",
        "        print(f\"Step-wise Floating Backward Removal Selected Features: {list(sfs_float_backward.k_feature_idx_)}\")\n",
        "        methods[f\"SFS_Float_Backward_{d}\"] = (X_train_sfsb, X_test_sfsb)\n",
        "    except Exception as e:\n",
        "        print(f\"Floating Backward Selection failed: {e}\")\n",
        "\n",
        "    # Principal Component Analysis (PCA)\n",
        "    try:\n",
        "        pca = PCA(n_components=d)\n",
        "        X_train_pca = pca.fit_transform(X_train)\n",
        "        X_test_pca = pca.transform(X_test)\n",
        "        print(f\"PCA Explained Variance Ratio: {pca.explained_variance_ratio_}\")\n",
        "        methods[f\"PCA_{d}\"] = (X_train_pca, X_test_pca)\n",
        "    except Exception as e:\n",
        "        print(f\"PCA failed: {e}\")\n",
        "\n",
        "    # Linear Discriminant Analysis (LDA)\n",
        "    try:\n",
        "        n_components = min(d, len(np.unique(y_train)) - 1)  # LDA components <= classes - 1\n",
        "        if n_components > 0:  # Only perform LDA if we have enough classes\n",
        "            lda = LDA(n_components=n_components)\n",
        "            X_train_lda = lda.fit_transform(X_train, y_train)\n",
        "            X_test_lda = lda.transform(X_test)\n",
        "            if hasattr(lda, 'explained_variance_ratio_'):\n",
        "                print(f\"LDA Explained Variance Ratio: {lda.explained_variance_ratio_}\")\n",
        "            methods[f\"LDA_{d}\"] = (X_train_lda, X_test_lda)\n",
        "    except Exception as e:\n",
        "        print(f\"LDA failed: {e}\")\n",
        "\n",
        "# Evaluate classifiers on each method\n",
        "results = {}\n",
        "for method_name, (X_tr, X_te) in methods.items():\n",
        "    results[method_name] = {}\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        try:\n",
        "            results[method_name][clf_name] = evaluate_model(clf, X_tr, X_te, y_train, y_test)\n",
        "            print(f\"Evaluated {clf_name} with {method_name}: {results[method_name][clf_name]['accuracy']:.4f} accuracy\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating {clf_name} with {method_name}: {e}\")\n",
        "            results[method_name][clf_name] = {\"accuracy\": 0, \"precision\": 0, \"recall\": 0, \"f1_score\": 0}\n",
        "\n",
        "# Convert results to DataFrame for analysis\n",
        "results_df = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
        "                                  for i in results.keys()\n",
        "                                  for j in results[i].keys()},\n",
        "                                  orient='index')\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results_df.to_csv(\"lung_cancer_metrics.csv\")\n",
        "print(\"Feature selection and dimensionality reduction completed.\")\n",
        "print(\"Results saved to 'lung_cancer_metrics.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2YK0Jw7NoIY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Function to evaluate a model\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"f1_score\": f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    }\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"ANN\": MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Evaluate classifiers for each dimensionality reduction/feature selection method\n",
        "for method_name, (X_tr, X_te) in methods.items():\n",
        "    results[method_name] = {}\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        try:\n",
        "            results[method_name][clf_name] = evaluate_model(clf, X_tr, X_te, y_train, y_test)\n",
        "            print(f\"Evaluated {clf_name} with {method_name}: {results[method_name][clf_name]['accuracy']:.4f} accuracy\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating {clf_name} with {method_name}: {e}\")\n",
        "            results[method_name][clf_name] = {\"accuracy\": 0, \"precision\": 0, \"recall\": 0, \"f1_score\": 0}\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame.from_dict({(i, j): results[i][j] for i in results.keys() for j in results[i].keys()},\n",
        "                                    orient='index')\n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv(\"lung_cancer_metrics.csv\")\n",
        "print(\"Model evaluation completed. Results saved to 'lung_cancer_metrics.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0kVPm_nPHHe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the performance metrics data\n",
        "results_df = pd.read_csv(\"lung_cancer_metrics.csv\", index_col=[0, 1])\n",
        "\n",
        "# Ensure the index is a MultiIndex\n",
        "if not isinstance(results_df.index, pd.MultiIndex):\n",
        "    results_df.index = pd.MultiIndex.from_tuples(results_df.index)\n",
        "\n",
        "# Check the structure of the DataFrame\n",
        "print(\"DataFrame Structure:\")\n",
        "print(results_df.index)\n",
        "print(results_df.columns)\n",
        "\n",
        "# Table Summary\n",
        "print(\"Table Summary of Performance Metrics:\")\n",
        "print(results_df)\n",
        "\n",
        "# Function to plot pie charts\n",
        "def plot_pie_charts(results_df, metric):\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(18, 18))\n",
        "    fig.suptitle(f'Pie Charts for {metric.capitalize()}', fontsize=20)\n",
        "    for idx, (method, data) in enumerate(results_df.groupby(level=0)):\n",
        "        if idx >= 9:\n",
        "            break  # Prevent exceeding subplot grid\n",
        "        ax = axes[idx // 3, idx % 3]\n",
        "        data = data.xs(method, level=0)[metric]  # Access data using xs with level=0\n",
        "        data.plot.pie(ax=ax, autopct='%1.1f%%', title=f'{method}')\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot bar charts\n",
        "def plot_bar_charts(results_df, metric):\n",
        "    results_metric = results_df[metric]  # Directly index if it's not a MultiIndex\n",
        "    results_metric.unstack(level=1).plot(kind='bar', figsize=(15, 10), title=f'Bar Charts for {metric.capitalize()}')\n",
        "    plt.ylabel(metric)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Classifiers', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot line graphs\n",
        "def plot_line_graphs(results_df, metric):\n",
        "    results_metric = results_df[metric]  # Directly index if it's not a MultiIndex\n",
        "    results_metric.unstack(level=1).plot(kind='line', marker='o', figsize=(15, 10), title=f'Line Graphs for {metric.capitalize()}')\n",
        "    plt.ylabel(metric)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Classifiers', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot box plots\n",
        "def plot_box_plots(results_df, metric):\n",
        "    results_metric = results_df[metric]  # Directly index if it's not a MultiIndex\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    sns.boxplot(data=results_metric.unstack(level=1))\n",
        "    plt.title(f'Box Plots for {metric.capitalize()}')\n",
        "    plt.ylabel(metric)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate plots for each metric\n",
        "metrics = [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n",
        "for metric in metrics:\n",
        "    plot_pie_charts(results_df, metric)\n",
        "    plot_bar_charts(results_df, metric)\n",
        "    plot_line_graphs(results_df, metric)\n",
        "    plot_box_plots(results_df, metric)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cGVo8AkYCFb"
      },
      "source": [
        "DATASET 4 : MUSHROOM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCyV5FmSYHUx",
        "outputId": "704ce495-5d5b-4d5a-9fdf-3a296681f529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 1 ###\n",
            "\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 2 ###\n",
            "\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 3 ###\n",
            "\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 4 ###\n",
            "\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 5 ###\n",
            "\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 6 ###\n",
            "\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 7 ###\n",
            "\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 8 ###\n",
            "\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 9 ###\n",
            "\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 10 ###\n",
            "\n",
            "Feature selection and dimensionality reduction completed.\n",
            "Results saved to 'mushroom_performance_metrics.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# Fetch dataset\n",
        "mushroom = fetch_ucirepo(id=73)\n",
        "\n",
        "# Data (as pandas dataframes)\n",
        "X = mushroom.data.features\n",
        "y = mushroom.data.targets\n",
        "\n",
        "# Encode categorical features\n",
        "X_encoded = pd.get_dummies(X)\n",
        "\n",
        "# Encode target variable\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y.iloc[:, 0])\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_encoded)\n",
        "\n",
        "# Splitting data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression as the base classifier\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Function to evaluate and collect performance metrics\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"f1_score\": f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    }\n",
        "\n",
        "# Classifiers to evaluate\n",
        "classifiers = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"ANN\": MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "# Dimensionality reduction methods\n",
        "methods = {}\n",
        "for d in range(1, min(11, X_encoded.shape[1])):\n",
        "    print(f\"\\n### Feature Selection and Dimensionality Reduction with d = {d} ###\\n\")\n",
        "\n",
        "    # Step-wise Forward Selection\n",
        "    sfs_forward = SequentialFeatureSelector(model, n_features_to_select=d, direction='forward').fit(X_train, y_train)\n",
        "    X_train_sfs = sfs_forward.transform(X_train)\n",
        "    X_test_sfs = sfs_forward.transform(X_test)\n",
        "    methods[f\"SFS_Forward_{d}\"] = (X_train_sfs, X_test_sfs)\n",
        "\n",
        "    # Principal Component Analysis (PCA)\n",
        "    pca = PCA(n_components=d)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    methods[f\"PCA_{d}\"] = (X_train_pca, X_test_pca)\n",
        "\n",
        "    # Linear Discriminant Analysis (LDA)\n",
        "    n_components = min(d, len(np.unique(y_train)) - 1)\n",
        "    if n_components > 0:\n",
        "        lda = LDA(n_components=n_components)\n",
        "        X_train_lda = lda.fit_transform(X_train, y_train)\n",
        "        X_test_lda = lda.transform(X_test)\n",
        "        methods[f\"LDA_{d}\"] = (X_train_lda, X_test_lda)\n",
        "\n",
        "# Evaluate classifiers on each method\n",
        "results = {}\n",
        "for method_name, (X_tr, X_te) in methods.items():\n",
        "    results[method_name] = {}\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        results[method_name][clf_name] = evaluate_model(clf, X_tr, X_te, y_train, y_test)\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame.from_dict({(i, j): results[i][j]\n",
        "                                     for i in results.keys()\n",
        "                                     for j in results[i].keys()},\n",
        "                                     orient='index')\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results_df.to_csv(\"mushroom_performance_metrics.csv\")\n",
        "\n",
        "print(\"Feature selection and dimensionality reduction completed.\")\n",
        "print(\"Results saved to 'mushroom_performance_metrics.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtdywAk8dN_o",
        "outputId": "81189173-fb27-41c9-f508-ee49cf990aae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluated Decision Tree with SFS_Forward_1: 0.8782 accuracy\n",
            "Evaluated KNN with SFS_Forward_1: 0.8782 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_1: 0.8782 accuracy\n",
            "Evaluated SVM with SFS_Forward_1: 0.8782 accuracy\n",
            "Evaluated ANN with SFS_Forward_1: 0.8782 accuracy\n",
            "Evaluated Decision Tree with PCA_1: 0.8535 accuracy\n",
            "Evaluated KNN with PCA_1: 0.8917 accuracy\n",
            "Evaluated Naive Bayes with PCA_1: 0.8868 accuracy\n",
            "Evaluated SVM with PCA_1: 0.8843 accuracy\n",
            "Evaluated ANN with PCA_1: 0.9065 accuracy\n",
            "Evaluated Decision Tree with LDA_1: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_1: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_1: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_1: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_1: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_2: 0.9360 accuracy\n",
            "Evaluated KNN with SFS_Forward_2: 0.9360 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_2: 0.9354 accuracy\n",
            "Evaluated SVM with SFS_Forward_2: 0.9360 accuracy\n",
            "Evaluated ANN with SFS_Forward_2: 0.9360 accuracy\n",
            "Evaluated Decision Tree with PCA_2: 0.9255 accuracy\n",
            "Evaluated KNN with PCA_2: 0.9366 accuracy\n",
            "Evaluated Naive Bayes with PCA_2: 0.8578 accuracy\n",
            "Evaluated SVM with PCA_2: 0.8948 accuracy\n",
            "Evaluated ANN with PCA_2: 0.9298 accuracy\n",
            "Evaluated Decision Tree with LDA_2: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_2: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_2: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_2: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_2: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_3: 0.9655 accuracy\n",
            "Evaluated KNN with SFS_Forward_3: 0.9655 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_3: 0.9649 accuracy\n",
            "Evaluated SVM with SFS_Forward_3: 0.9655 accuracy\n",
            "Evaluated ANN with SFS_Forward_3: 0.9655 accuracy\n",
            "Evaluated Decision Tree with PCA_3: 0.9711 accuracy\n",
            "Evaluated KNN with PCA_3: 0.9828 accuracy\n",
            "Evaluated Naive Bayes with PCA_3: 0.8800 accuracy\n",
            "Evaluated SVM with PCA_3: 0.9003 accuracy\n",
            "Evaluated ANN with PCA_3: 0.9637 accuracy\n",
            "Evaluated Decision Tree with LDA_3: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_3: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_3: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_3: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_3: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_4: 0.9742 accuracy\n",
            "Evaluated KNN with SFS_Forward_4: 0.9742 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_4: 0.9735 accuracy\n",
            "Evaluated SVM with SFS_Forward_4: 0.9742 accuracy\n",
            "Evaluated ANN with SFS_Forward_4: 0.9742 accuracy\n",
            "Evaluated Decision Tree with PCA_4: 0.9938 accuracy\n",
            "Evaluated KNN with PCA_4: 0.9963 accuracy\n",
            "Evaluated Naive Bayes with PCA_4: 0.8935 accuracy\n",
            "Evaluated SVM with PCA_4: 0.9502 accuracy\n",
            "Evaluated ANN with PCA_4: 0.9926 accuracy\n",
            "Evaluated Decision Tree with LDA_4: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_4: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_4: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_4: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_4: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_5: 0.9809 accuracy\n",
            "Evaluated KNN with SFS_Forward_5: 0.9772 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_5: 0.9735 accuracy\n",
            "Evaluated SVM with SFS_Forward_5: 0.9809 accuracy\n",
            "Evaluated ANN with SFS_Forward_5: 0.9809 accuracy\n",
            "Evaluated Decision Tree with PCA_5: 0.9957 accuracy\n",
            "Evaluated KNN with PCA_5: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with PCA_5: 0.8935 accuracy\n",
            "Evaluated SVM with PCA_5: 0.9514 accuracy\n",
            "Evaluated ANN with PCA_5: 0.9975 accuracy\n",
            "Evaluated Decision Tree with LDA_5: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_5: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_5: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_5: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_5: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_6: 0.9889 accuracy\n",
            "Evaluated KNN with SFS_Forward_6: 0.9852 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_6: 0.9815 accuracy\n",
            "Evaluated SVM with SFS_Forward_6: 0.9889 accuracy\n",
            "Evaluated ANN with SFS_Forward_6: 0.9889 accuracy\n",
            "Evaluated Decision Tree with PCA_6: 0.9988 accuracy\n",
            "Evaluated KNN with PCA_6: 0.9994 accuracy\n",
            "Evaluated Naive Bayes with PCA_6: 0.8942 accuracy\n",
            "Evaluated SVM with PCA_6: 0.9618 accuracy\n",
            "Evaluated ANN with PCA_6: 0.9982 accuracy\n",
            "Evaluated Decision Tree with LDA_6: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_6: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_6: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_6: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_6: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_7: 0.9975 accuracy\n",
            "Evaluated KNN with SFS_Forward_7: 0.9938 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_7: 0.5803 accuracy\n",
            "Evaluated SVM with SFS_Forward_7: 0.9975 accuracy\n",
            "Evaluated ANN with SFS_Forward_7: 0.9975 accuracy\n",
            "Evaluated Decision Tree with PCA_7: 0.9975 accuracy\n",
            "Evaluated KNN with PCA_7: 0.9988 accuracy\n",
            "Evaluated Naive Bayes with PCA_7: 0.9102 accuracy\n",
            "Evaluated SVM with PCA_7: 0.9735 accuracy\n",
            "Evaluated ANN with PCA_7: 0.9994 accuracy\n",
            "Evaluated Decision Tree with LDA_7: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_7: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_7: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_7: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_7: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_8: 0.9994 accuracy\n",
            "Evaluated KNN with SFS_Forward_8: 0.9994 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_8: 0.5803 accuracy\n",
            "Evaluated SVM with SFS_Forward_8: 0.9994 accuracy\n",
            "Evaluated ANN with SFS_Forward_8: 0.9994 accuracy\n",
            "Evaluated Decision Tree with PCA_8: 0.9994 accuracy\n",
            "Evaluated KNN with PCA_8: 0.9988 accuracy\n",
            "Evaluated Naive Bayes with PCA_8: 0.9077 accuracy\n",
            "Evaluated SVM with PCA_8: 0.9822 accuracy\n",
            "Evaluated ANN with PCA_8: 0.9994 accuracy\n",
            "Evaluated Decision Tree with LDA_8: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_8: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_8: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_8: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_8: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_9: 1.0000 accuracy\n",
            "Evaluated KNN with SFS_Forward_9: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_9: 0.9618 accuracy\n",
            "Evaluated SVM with SFS_Forward_9: 1.0000 accuracy\n",
            "Evaluated ANN with SFS_Forward_9: 1.0000 accuracy\n",
            "Evaluated Decision Tree with PCA_9: 0.9982 accuracy\n",
            "Evaluated KNN with PCA_9: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with PCA_9: 0.9077 accuracy\n",
            "Evaluated SVM with PCA_9: 0.9951 accuracy\n",
            "Evaluated ANN with PCA_9: 0.9994 accuracy\n",
            "Evaluated Decision Tree with LDA_9: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_9: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_9: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_9: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_9: 1.0000 accuracy\n",
            "Evaluated Decision Tree with SFS_Forward_10: 1.0000 accuracy\n",
            "Evaluated KNN with SFS_Forward_10: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with SFS_Forward_10: 0.9618 accuracy\n",
            "Evaluated SVM with SFS_Forward_10: 0.9994 accuracy\n",
            "Evaluated ANN with SFS_Forward_10: 1.0000 accuracy\n",
            "Evaluated Decision Tree with PCA_10: 0.9969 accuracy\n",
            "Evaluated KNN with PCA_10: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with PCA_10: 0.9169 accuracy\n",
            "Evaluated SVM with PCA_10: 0.9988 accuracy\n",
            "Evaluated ANN with PCA_10: 0.9994 accuracy\n",
            "Evaluated Decision Tree with LDA_10: 1.0000 accuracy\n",
            "Evaluated KNN with LDA_10: 1.0000 accuracy\n",
            "Evaluated Naive Bayes with LDA_10: 1.0000 accuracy\n",
            "Evaluated SVM with LDA_10: 1.0000 accuracy\n",
            "Evaluated ANN with LDA_10: 1.0000 accuracy\n",
            "Model evaluation completed. Results saved to 'mushroom_classification_metrics.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Function to evaluate and collect performance metrics\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"f1_score\": f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    }\n",
        "\n",
        "# Classifiers to evaluate\n",
        "classifiers = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"ANN\": MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "# Evaluate classifiers on each method\n",
        "results = {}\n",
        "for method_name, (X_tr, X_te) in methods.items():\n",
        "    results[method_name] = {}\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        try:\n",
        "            results[method_name][clf_name] = evaluate_model(clf, X_tr, X_te, y_train, y_test)\n",
        "            print(f\"Evaluated {clf_name} with {method_name}: {results[method_name][clf_name]['accuracy']:.4f} accuracy\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating {clf_name} with {method_name}: {e}\")\n",
        "            results[method_name][clf_name] = {\"accuracy\": 0, \"precision\": 0, \"recall\": 0, \"f1_score\": 0}\n",
        "\n",
        "# Convert results to DataFrame for analysis\n",
        "results_df = pd.DataFrame.from_dict({(i, j): results[i][j] for i in results.keys() for j in results[i].keys()},\n",
        "                                   orient='index')\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results_df.to_csv(\"mushroom_classification_metrics.csv\")\n",
        "\n",
        "print(\"Model evaluation completed. Results saved to 'mushroom_classification_metrics.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvkwP6CGdh2P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Load the performance metrics data\n",
        "results_df = pd.read_csv(\"mushroom_classification_metrics.csv\", index_col=[0, 1])\n",
        "\n",
        "# Ensure the index is a MultiIndex\n",
        "if not isinstance(results_df.index, pd.MultiIndex):\n",
        "    results_df.index = pd.MultiIndex.from_tuples(results_df.index)\n",
        "\n",
        "# Check the structure of the DataFrame\n",
        "print(\"DataFrame Structure:\")\n",
        "print(results_df.index)\n",
        "print(results_df.columns)\n",
        "\n",
        "# Table Summary\n",
        "print(\"Table Summary of Performance Metrics:\")\n",
        "print(results_df)\n",
        "\n",
        "# Pie Charts\n",
        "def plot_pie_charts(results_df, metric):\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(18, 18))\n",
        "    fig.suptitle(f'Pie Charts for {metric.capitalize()}', fontsize=20)\n",
        "    for idx, (method, data) in enumerate(results_df.groupby(level=0)):\n",
        "        if idx >= 9:\n",
        "            break\n",
        "        ax = axes[idx // 3, idx % 3]\n",
        "        data = data.xs(method, level=0)[metric]\n",
        "        data.plot.pie(ax=ax, autopct='%1.1f%%', title=f'{method}')\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_pie_charts(results_df, metric)\n",
        "\n",
        "# Bar Charts\n",
        "def plot_bar_charts(results_df, metric):\n",
        "    results_metric = results_df[metric]\n",
        "    results_metric.unstack(level=1).plot(kind='bar', figsize=(15, 10), title=f'Bar Charts for {metric.capitalize()}')\n",
        "    plt.ylabel(metric)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Classifiers', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_bar_charts(results_df, metric)\n",
        "\n",
        "# Line Graphs\n",
        "def plot_line_graphs(results_df, metric):\n",
        "    results_metric = results_df[metric]\n",
        "    results_metric.unstack(level=1).plot(kind='line', marker='o', figsize=(15, 10), title=f'Line Graphs for {metric.capitalize()}')\n",
        "    plt.ylabel(metric)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Classifiers', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_line_graphs(results_df, metric)\n",
        "\n",
        "# Box Plots\n",
        "def plot_box_plots(results_df, metric):\n",
        "    results_metric = results_df[metric]\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    sns.boxplot(data=results_metric.unstack(level=1))\n",
        "    plt.title(f'Box Plots for {metric.capitalize()}')\n",
        "    plt.ylabel(metric)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_box_plots(results_df, metric)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkCA95P69aMR"
      },
      "source": [
        "DATASET 5 : SPAMBASE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3vX0DKD9Zg7",
        "outputId": "450d630d-f2d5-4111-a5e3-c3366f2bf1e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values in original X: 0\n",
            "Missing values in original y: 0\n",
            "Missing values after preprocessing: 0\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 1 ###\n",
            "\n",
            "Naïve Search Selected Features: [9]\n",
            "Step-wise Forward Selection Selected Features: [52]\n",
            "Step-wise Backward Removal Selected Features: [52]\n",
            "Bidirectional Search Selected Features: [52]\n",
            "Step-wise Floating Forward Selection Selected Features: [52]\n",
            "Step-wise Floating Backward Removal Selected Features: [52]\n",
            "PCA Explained Variance Ratio: [0.10776224]\n",
            "LDA Explained Variance Ratio: [1.]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 2 ###\n",
            "\n",
            "Naïve Search Selected Features: [ 5 32]\n",
            "Step-wise Forward Selection Selected Features: [ 6 52]\n",
            "Step-wise Backward Removal Selected Features: [ 6 52]\n",
            "Bidirectional Search Selected Features: [6, 52]\n",
            "Step-wise Floating Forward Selection Selected Features: [6, 52]\n",
            "Step-wise Floating Backward Removal Selected Features: [6, 52]\n",
            "PCA Explained Variance Ratio: [0.10776224 0.058507  ]\n",
            "LDA Explained Variance Ratio: [1.]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 3 ###\n",
            "\n",
            "Naïve Search Selected Features: [54 14  7]\n",
            "Step-wise Forward Selection Selected Features: [ 6 24 52]\n",
            "Step-wise Backward Removal Selected Features: [ 6 24 52]\n",
            "Bidirectional Search Selected Features: [6, 24, 52]\n",
            "Step-wise Floating Forward Selection Selected Features: [6, 24, 52]\n",
            "Step-wise Floating Backward Removal Selected Features: [6, 24, 52]\n",
            "PCA Explained Variance Ratio: [0.10776224 0.058507   0.03819489]\n",
            "LDA Explained Variance Ratio: [1.]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 4 ###\n",
            "\n",
            "Naïve Search Selected Features: [30  1 55  4]\n",
            "Step-wise Forward Selection Selected Features: [ 6 24 52 55]\n",
            "Step-wise Backward Removal Selected Features: [ 6 15 24 52]\n",
            "Bidirectional Search Selected Features: [6, 24, 52, 55]\n",
            "Step-wise Floating Forward Selection Selected Features: [6, 24, 52, 55]\n",
            "Step-wise Floating Backward Removal Selected Features: [6, 15, 24, 52]\n",
            "PCA Explained Variance Ratio: [0.10776224 0.058507   0.03819489 0.02952557]\n",
            "LDA Explained Variance Ratio: [1.]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 5 ###\n",
            "\n",
            "Naïve Search Selected Features: [ 2  3 33  9 45]\n",
            "Step-wise Forward Selection Selected Features: [ 6 22 24 52 55]\n",
            "Step-wise Backward Removal Selected Features: [ 6 15 24 26 52]\n",
            "Bidirectional Search Selected Features: [6, 22, 24, 52, 55]\n",
            "Step-wise Floating Forward Selection Selected Features: [6, 22, 24, 52, 55]\n",
            "Step-wise Floating Backward Removal Selected Features: [6, 15, 24, 26, 52]\n",
            "PCA Explained Variance Ratio: [0.10776224 0.058507   0.03819489 0.02952557 0.02822727]\n",
            "LDA Explained Variance Ratio: [1.]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 6 ###\n",
            "\n",
            "Naïve Search Selected Features: [11  5 23 31 19 36]\n",
            "Step-wise Forward Selection Selected Features: [ 6 16 22 24 52 55]\n",
            "Step-wise Backward Removal Selected Features: [ 6 15 24 26 45 52]\n",
            "Bidirectional Search Selected Features: [6, 16, 22, 24, 52, 55]\n",
            "Step-wise Floating Forward Selection Selected Features: [6, 16, 22, 24, 52, 55]\n",
            "Step-wise Floating Backward Removal Selected Features: [6, 15, 24, 26, 45, 52]\n",
            "PCA Explained Variance Ratio: [0.10776224 0.058507   0.03819489 0.02952557 0.02822727 0.02629532]\n",
            "LDA Explained Variance Ratio: [1.]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 7 ###\n",
            "\n",
            "Naïve Search Selected Features: [33 39  4  9  7 38 48]\n",
            "Step-wise Forward Selection Selected Features: [ 6 16 22 24 45 52 55]\n",
            "Step-wise Backward Removal Selected Features: [ 6 15 24 26 41 45 52]\n",
            "Bidirectional Search Selected Features: [6, 16, 22, 24, 45, 52, 55]\n",
            "Step-wise Floating Forward Selection Selected Features: [6, 16, 22, 24, 45, 52, 55]\n",
            "Step-wise Floating Backward Removal Selected Features: [6, 15, 16, 24, 26, 45, 52]\n",
            "PCA Explained Variance Ratio: [0.10776224 0.058507   0.03819489 0.02952557 0.02822727 0.02629532\n",
            " 0.02546964]\n",
            "LDA Explained Variance Ratio: [1.]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 8 ###\n",
            "\n",
            "Naïve Search Selected Features: [18  0 21  8 27  2 53 51]\n",
            "Step-wise Forward Selection Selected Features: [ 6 16 22 24 45 51 52 55]\n",
            "Step-wise Backward Removal Selected Features: [ 6 15 24 26 41 45 52 56]\n",
            "Bidirectional Search Selected Features: [6, 16, 22, 24, 45, 51, 52, 55]\n",
            "Step-wise Floating Forward Selection Selected Features: [6, 16, 22, 24, 45, 51, 52, 55]\n",
            "Step-wise Floating Backward Removal Selected Features: [6, 15, 16, 22, 24, 26, 45, 52]\n",
            "PCA Explained Variance Ratio: [0.10776224 0.058507   0.03819489 0.02952557 0.02822727 0.02629532\n",
            " 0.02546964 0.0239715 ]\n",
            "LDA Explained Variance Ratio: [1.]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 9 ###\n",
            "\n",
            "Naïve Search Selected Features: [37  6 41 22  3 46 23 16 18]\n",
            "Step-wise Forward Selection Selected Features: [ 6 16 22 24 41 45 51 52 55]\n",
            "Step-wise Backward Removal Selected Features: [ 6 15 24 26 41 45 51 52 56]\n",
            "Bidirectional Search Selected Features: [6, 16, 22, 24, 41, 45, 51, 52, 55]\n",
            "Step-wise Floating Forward Selection Selected Features: [6, 16, 22, 24, 41, 45, 51, 52, 55]\n",
            "Step-wise Floating Backward Removal Selected Features: [6, 15, 16, 22, 24, 26, 41, 45, 52]\n",
            "PCA Explained Variance Ratio: [0.10776224 0.058507   0.03819489 0.02952557 0.02822727 0.02629532\n",
            " 0.02546964 0.0239715  0.02321614]\n",
            "LDA Explained Variance Ratio: [1.]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 10 ###\n",
            "\n",
            "Naïve Search Selected Features: [36 45 41 22  2 32 39 34  7 50]\n",
            "Step-wise Forward Selection Selected Features: [ 6 15 16 22 24 41 45 51 52 55]\n",
            "Step-wise Backward Removal Selected Features: [ 4  6 15 24 26 41 45 51 52 56]\n",
            "Bidirectional Search Selected Features: [6, 15, 16, 22, 24, 41, 45, 51, 52, 55]\n",
            "Step-wise Floating Forward Selection Selected Features: [6, 15, 16, 22, 24, 41, 45, 51, 52, 55]\n",
            "Step-wise Floating Backward Removal Selected Features: [6, 15, 16, 22, 24, 26, 41, 45, 52, 55]\n",
            "PCA Explained Variance Ratio: [0.10776224 0.058507   0.03819489 0.02952557 0.02822727 0.02629532\n",
            " 0.02546964 0.0239715  0.02321614 0.02285333]\n",
            "LDA Explained Variance Ratio: [1.]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 11 ###\n",
            "\n",
            "Naïve Search Selected Features: [ 5 53 10 27 30 51 31 14 41 24 13]\n",
            "Step-wise Forward Selection Selected Features: [ 6 15 16 22 24 26 41 45 51 52 55]\n",
            "Step-wise Backward Removal Selected Features: [ 4  6 15 24 26 32 41 45 51 52 56]\n",
            "Bidirectional Search Selected Features: [6, 15, 16, 22, 24, 26, 41, 45, 51, 52, 55]\n",
            "Step-wise Floating Forward Selection Selected Features: [4, 6, 15, 22, 24, 26, 41, 45, 51, 52, 55]\n",
            "Step-wise Floating Backward Removal Selected Features: [6, 15, 16, 22, 24, 26, 30, 41, 45, 52, 55]\n",
            "PCA Explained Variance Ratio: [0.10776224 0.058507   0.03819489 0.02952557 0.02822727 0.02629532\n",
            " 0.02546964 0.0239715  0.02321614 0.02285333 0.02161731]\n",
            "LDA Explained Variance Ratio: [1.]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 12 ###\n",
            "\n",
            "Naïve Search Selected Features: [24 25 55 12 54 23 17 28 20  7 48  6]\n",
            "Step-wise Forward Selection Selected Features: [ 6 15 16 22 24 26 39 41 45 51 52 55]\n",
            "Step-wise Backward Removal Selected Features: [ 4  6 15 24 26 32 41 43 45 51 52 56]\n",
            "Bidirectional Search Selected Features: [6, 15, 16, 22, 24, 26, 39, 41, 45, 51, 52, 55]\n",
            "Step-wise Floating Forward Selection Selected Features: [4, 6, 7, 15, 22, 24, 26, 41, 45, 51, 52, 55]\n",
            "Step-wise Floating Backward Removal Selected Features: [6, 15, 16, 18, 22, 24, 26, 41, 44, 45, 52, 55]\n",
            "PCA Explained Variance Ratio: [0.10776224 0.058507   0.03819489 0.02952557 0.02822727 0.02629532\n",
            " 0.02546964 0.0239715  0.02321614 0.02285333 0.02161731 0.02134173]\n",
            "LDA Explained Variance Ratio: [1.]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 13 ###\n",
            "\n",
            "Naïve Search Selected Features: [40 53 24 45 51 11 23 31  8 29 42  7 13]\n",
            "Step-wise Forward Selection Selected Features: [ 6  7 15 16 22 24 26 39 41 45 51 52 55]\n",
            "Step-wise Backward Removal Selected Features: [ 4  6 15 16 24 26 32 41 43 45 51 52 56]\n",
            "Bidirectional Search Selected Features: [6, 7, 15, 16, 22, 24, 26, 39, 41, 45, 51, 52, 55]\n",
            "Step-wise Floating Forward Selection Selected Features: [4, 6, 7, 15, 22, 24, 26, 41, 43, 45, 51, 52, 55]\n",
            "Step-wise Floating Backward Removal Selected Features: [6, 15, 16, 18, 22, 24, 26, 41, 44, 45, 51, 52, 55]\n",
            "PCA Explained Variance Ratio: [0.10776224 0.058507   0.03819489 0.02952557 0.02822727 0.02629532\n",
            " 0.02546964 0.0239715  0.02321614 0.02285333 0.02161731 0.02134173\n",
            " 0.02125061]\n",
            "LDA Explained Variance Ratio: [1.]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 14 ###\n",
            "\n",
            "Naïve Search Selected Features: [31 39 16 15 42 38 51 55  0 21 22 56 24 27]\n",
            "Step-wise Forward Selection Selected Features: [ 6  7 15 16 22 24 26 36 39 41 45 51 52 55]\n",
            "Step-wise Backward Removal Selected Features: [ 4  6  7 15 16 24 26 32 41 43 45 51 52 56]\n",
            "Bidirectional Search Selected Features: [6, 7, 15, 16, 22, 24, 26, 36, 39, 41, 45, 51, 52, 55]\n",
            "Step-wise Floating Forward Selection Selected Features: [4, 6, 7, 10, 15, 22, 24, 26, 41, 43, 45, 51, 52, 55]\n",
            "Step-wise Floating Backward Removal Selected Features: [6, 15, 16, 18, 22, 24, 26, 30, 41, 44, 45, 51, 52, 55]\n",
            "PCA Explained Variance Ratio: [0.10776224 0.058507   0.03819489 0.02952557 0.02822727 0.02629532\n",
            " 0.02546964 0.0239715  0.02321614 0.02285333 0.02161731 0.02134173\n",
            " 0.02125061 0.0202774 ]\n",
            "LDA Explained Variance Ratio: [1.]\n",
            "\n",
            "### Feature Selection and Dimensionality Reduction with d = 15 ###\n",
            "\n",
            "Naïve Search Selected Features: [38  0 50 55  7 56 47 22 53 19 44 29  1 25 41]\n",
            "Step-wise Forward Selection Selected Features: [ 6  7 15 16 22 24 26 36 37 39 41 45 51 52 55]\n",
            "Step-wise Backward Removal Selected Features: [ 4  6  7  8 15 16 24 26 32 41 43 45 51 52 56]\n",
            "Bidirectional Search Selected Features: [6, 7, 15, 16, 22, 24, 26, 36, 37, 39, 41, 45, 51, 52, 55]\n",
            "Step-wise Floating Forward Selection Selected Features: [4, 6, 7, 10, 15, 22, 24, 26, 38, 41, 43, 45, 51, 52, 55]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch Spambase dataset\n",
        "spambase = fetch_ucirepo(id=94)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = spambase.data.features\n",
        "y = spambase.data.targets\n",
        "\n",
        "# Create a copy of X and y to avoid the SettingWithCopyWarning\n",
        "X_copy = X.copy()\n",
        "y_copy = y.copy()\n",
        "\n",
        "# Check for missing values before preprocessing\n",
        "print(f\"Missing values in original X: {X_copy.isnull().sum().sum()}\")\n",
        "print(f\"Missing values in original y: {y_copy.isnull().sum().sum()}\")\n",
        "\n",
        "# Handle missing values properly\n",
        "X_copy = X_copy.replace('?', np.nan)\n",
        "X_copy = X_copy.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill remaining missing values with the mean of each column\n",
        "for col in X_copy.columns:\n",
        "    if X_copy[col].isnull().sum() > 0:\n",
        "        X_copy[col] = X_copy[col].fillna(X_copy[col].mean())\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(f\"Missing values after preprocessing: {X_copy.isnull().sum().sum()}\")\n",
        "\n",
        "# For the spambase dataset, we use the 'spam' column as our target\n",
        "# This dataset already has a binary target (1 for spam, 0 for non-spam)\n",
        "target = y_copy.iloc[:, 0]\n",
        "\n",
        "# Make sure target has no missing values\n",
        "if target.isnull().sum() > 0:\n",
        "    target = target.fillna(target.mode()[0])  # Fill with most common value\n",
        "\n",
        "# Standardizing features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_copy)\n",
        "\n",
        "# Double-check for NaN values after scaling\n",
        "if np.isnan(X_scaled).any():\n",
        "    print(\"Warning: NaN values found after scaling. Replacing with 0...\")\n",
        "    X_scaled = np.nan_to_num(X_scaled)\n",
        "\n",
        "# Splitting data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression as the base classifier\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Function to evaluate and collect performance metrics\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"f1_score\": f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    }\n",
        "\n",
        "# Classifiers to evaluate\n",
        "classifiers = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"ANN\": MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "# Dimensionality reduction methods\n",
        "methods = {}\n",
        "# Test a range of dimensions - spambase has 57 features, so let's try up to 15\n",
        "for d in range(1, min(16, X_copy.shape[1])):  # Up to 15 features or max available\n",
        "    print(f\"\\n### Feature Selection and Dimensionality Reduction with d = {d} ###\\n\")\n",
        "\n",
        "    # Naïve Search (Random selection of d features)\n",
        "    feature_indices = np.random.choice(range(X_scaled.shape[1]), d, replace=False)\n",
        "    X_train_naive = X_train[:, feature_indices]\n",
        "    X_test_naive = X_test[:, feature_indices]\n",
        "    print(f\"Naïve Search Selected Features: {feature_indices}\")\n",
        "    methods[f\"Naive_{d}\"] = (X_train_naive, X_test_naive)\n",
        "\n",
        "    try:\n",
        "        # Step-wise Forward Selection\n",
        "        sfs_forward = SequentialFeatureSelector(model, n_features_to_select=d, direction='forward').fit(X_train, y_train)\n",
        "        X_train_sfs = sfs_forward.transform(X_train)\n",
        "        X_test_sfs = sfs_forward.transform(X_test)\n",
        "        print(f\"Step-wise Forward Selection Selected Features: {np.where(sfs_forward.get_support())[0]}\")\n",
        "        methods[f\"SFS_Forward_{d}\"] = (X_train_sfs, X_test_sfs)\n",
        "    except Exception as e:\n",
        "        print(f\"Step-wise Forward Selection failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        # Step-wise Backward Removal\n",
        "        sfs_backward = SequentialFeatureSelector(model, n_features_to_select=d, direction='backward').fit(X_train, y_train)\n",
        "        X_train_sbs = sfs_backward.transform(X_train)\n",
        "        X_test_sbs = sfs_backward.transform(X_test)\n",
        "        print(f\"Step-wise Backward Removal Selected Features: {np.where(sfs_backward.get_support())[0]}\")\n",
        "        methods[f\"SFS_Backward_{d}\"] = (X_train_sbs, X_test_sbs)\n",
        "    except Exception as e:\n",
        "        print(f\"Step-wise Backward Removal failed: {e}\")\n",
        "\n",
        "    # Bidirectional Search (using MLxtend's SFS)\n",
        "    try:\n",
        "        sfs_bi = SFS(model, k_features=d, forward=True, floating=False, scoring='accuracy', cv=5).fit(X_train, y_train)\n",
        "        X_train_bi = X_train[:, list(sfs_bi.k_feature_idx_)]\n",
        "        X_test_bi = X_test[:, list(sfs_bi.k_feature_idx_)]\n",
        "        print(f\"Bidirectional Search Selected Features: {list(sfs_bi.k_feature_idx_)}\")\n",
        "        methods[f\"SFS_Bi_{d}\"] = (X_train_bi, X_test_bi)\n",
        "    except Exception as e:\n",
        "        print(f\"Bidirectional Search failed: {e}\")\n",
        "\n",
        "    # Step-wise Floating Forward Selection\n",
        "    try:\n",
        "        sfs_float_forward = SFS(model, k_features=d, forward=True, floating=True, scoring='accuracy', cv=5).fit(X_train, y_train)\n",
        "        X_train_sfsf = X_train[:, list(sfs_float_forward.k_feature_idx_)]\n",
        "        X_test_sfsf = X_test[:, list(sfs_float_forward.k_feature_idx_)]\n",
        "        print(f\"Step-wise Floating Forward Selection Selected Features: {list(sfs_float_forward.k_feature_idx_)}\")\n",
        "        methods[f\"SFS_Float_Forward_{d}\"] = (X_train_sfsf, X_test_sfsf)\n",
        "    except Exception as e:\n",
        "        print(f\"Floating Forward Selection failed: {e}\")\n",
        "\n",
        "    # Step-wise Floating Backward Removal\n",
        "    try:\n",
        "        sfs_float_backward = SFS(model, k_features=d, forward=False, floating=True, scoring='accuracy', cv=5).fit(X_train, y_train)\n",
        "        X_train_sfsb = X_train[:, list(sfs_float_backward.k_feature_idx_)]\n",
        "        X_test_sfsb = X_test[:, list(sfs_float_backward.k_feature_idx_)]\n",
        "        print(f\"Step-wise Floating Backward Removal Selected Features: {list(sfs_float_backward.k_feature_idx_)}\")\n",
        "        methods[f\"SFS_Float_Backward_{d}\"] = (X_train_sfsb, X_test_sfsb)\n",
        "    except Exception as e:\n",
        "        print(f\"Floating Backward Selection failed: {e}\")\n",
        "\n",
        "    # Principal Component Analysis (PCA)\n",
        "    try:\n",
        "        pca = PCA(n_components=d)\n",
        "        X_train_pca = pca.fit_transform(X_train)\n",
        "        X_test_pca = pca.transform(X_test)\n",
        "        print(f\"PCA Explained Variance Ratio: {pca.explained_variance_ratio_}\")\n",
        "        methods[f\"PCA_{d}\"] = (X_train_pca, X_test_pca)\n",
        "    except Exception as e:\n",
        "        print(f\"PCA failed: {e}\")\n",
        "\n",
        "    # Linear Discriminant Analysis (LDA)\n",
        "    try:\n",
        "        n_components = min(d, len(np.unique(y_train)) - 1)  # LDA components <= classes - 1\n",
        "        if n_components > 0:  # Only perform LDA if we have enough classes\n",
        "            lda = LDA(n_components=n_components)\n",
        "            X_train_lda = lda.fit_transform(X_train, y_train)\n",
        "            X_test_lda = lda.transform(X_test)\n",
        "            if hasattr(lda, 'explained_variance_ratio_'):\n",
        "                print(f\"LDA Explained Variance Ratio: {lda.explained_variance_ratio_}\")\n",
        "            methods[f\"LDA_{d}\"] = (X_train_lda, X_test_lda)\n",
        "    except Exception as e:\n",
        "        print(f\"LDA failed: {e}\")\n",
        "\n",
        "# Evaluate classifiers on each method\n",
        "results = {}\n",
        "for method_name, (X_tr, X_te) in methods.items():\n",
        "    results[method_name] = {}\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        try:\n",
        "            results[method_name][clf_name] = evaluate_model(clf, X_tr, X_te, y_train, y_test)\n",
        "            print(f\"Evaluated {clf_name} with {method_name}: {results[method_name][clf_name]['accuracy']:.4f} accuracy\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating {clf_name} with {method_name}: {e}\")\n",
        "            results[method_name][clf_name] = {\"accuracy\": 0, \"precision\": 0, \"recall\": 0, \"f1_score\": 0}\n",
        "\n",
        "# Convert results to DataFrame for analysis\n",
        "results_df = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
        "                                   for i in results.keys()\n",
        "                                   for j in results[i].keys()},\n",
        "                                   orient='index')\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results_df.to_csv(\"spambase_metrics.csv\")\n",
        "\n",
        "# Create a visualization of the top methods\n",
        "# Extract accuracy values for each method and classifier\n",
        "accuracy_df = pd.DataFrame(index=results.keys(), columns=classifiers.keys())\n",
        "for method in results.keys():\n",
        "    for clf in classifiers.keys():\n",
        "        accuracy_df.loc[method, clf] = results[method][clf]['accuracy']\n",
        "\n",
        "# Get the top 10 method-classifier combinations by accuracy\n",
        "top_combinations = accuracy_df.stack().sort_values(ascending=False).head(10)\n",
        "print(\"\\nTop 10 method-classifier combinations by accuracy:\")\n",
        "print(top_combinations)\n",
        "\n",
        "print(\"\\nFeature selection and dimensionality reduction completed.\")\n",
        "print(\"Results saved to 'spambase_metrics.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AKoBzqHELNx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Define function to evaluate classifiers with cross-validation\n",
        "def cross_val_evaluate(model, X, y, cv=5):\n",
        "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "    return scores.mean(), scores.std()\n",
        "\n",
        "# Define function to evaluate models and compute additional metrics\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else y_pred\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"f1_score\": f1_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "        \"roc_auc\": roc_auc_score(y_test, y_pred_proba) if len(set(y_test)) > 1 else 0\n",
        "    }\n",
        "\n",
        "# Define classifiers to evaluate\n",
        "classifiers = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"SVM\": SVC(probability=True),\n",
        "    \"ANN\": MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "# Evaluate classifiers on each feature selection method\n",
        "results = {}\n",
        "for method_name, (X_tr, X_te) in methods.items():\n",
        "    results[method_name] = {}\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        try:\n",
        "            results[method_name][clf_name] = evaluate_model(clf, X_tr, X_te, y_train, y_test)\n",
        "            print(f\"Evaluated {clf_name} with {method_name}: {results[method_name][clf_name]['accuracy']:.4f} accuracy\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating {clf_name} with {method_name}: {e}\")\n",
        "            results[method_name][clf_name] = {\"accuracy\": 0, \"precision\": 0, \"recall\": 0, \"f1_score\": 0, \"roc_auc\": 0}\n",
        "\n",
        "# Convert results to DataFrame for analysis\n",
        "results_df = pd.DataFrame.from_dict({(i, j): results[i][j] for i in results.keys() for j in results[i].keys()},\n",
        "                                    orient='index')\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results_df.to_csv(\"model_performance_metrics.csv\")\n",
        "\n",
        "print(\"Model evaluation completed. Results saved to 'model_performance_metrics.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Heh-oGhDERgu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the performance metrics data\n",
        "results_df = pd.read_csv(\"student_performance_metrics.csv\", index_col=[0, 1])\n",
        "\n",
        "# Ensure the index is a MultiIndex\n",
        "if not isinstance(results_df.index, pd.MultiIndex):\n",
        "    results_df.index = pd.MultiIndex.from_tuples(results_df.index)\n",
        "\n",
        "# Check structure\n",
        "print(\"DataFrame Structure:\")\n",
        "print(results_df.index)\n",
        "print(results_df.columns)\n",
        "\n",
        "# 4i) Table Summary\n",
        "print(\"Table Summary of Performance Metrics:\")\n",
        "print(results_df)\n",
        "\n",
        "# Define a function to handle subplots dynamically\n",
        "def create_subplots(num_plots, title):\n",
        "    rows = (num_plots // 3) + (num_plots % 3 > 0)\n",
        "    fig, axes = plt.subplots(rows, 3, figsize=(18, rows * 6))\n",
        "    fig.suptitle(title, fontsize=20)\n",
        "    axes = axes.flatten()\n",
        "    return fig, axes\n",
        "\n",
        "# 4ii) Pie Charts\n",
        "def plot_pie_charts(results_df, metric):\n",
        "    unique_methods = results_df.index.get_level_values(0).unique()\n",
        "    fig, axes = create_subplots(len(unique_methods), f'Pie Charts for {metric.capitalize()}')\n",
        "\n",
        "    for idx, method in enumerate(unique_methods):\n",
        "        data = results_df.xs(method, level=0)[metric]\n",
        "        data.plot.pie(ax=axes[idx], autopct='%1.1f%%', title=f'{method}')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_pie_charts(results_df, metric)\n",
        "\n",
        "# 4iii) Bar Charts\n",
        "def plot_bar_charts(results_df, metric):\n",
        "    results_metric = results_df[metric].unstack(level=1)\n",
        "    results_metric.plot(kind='bar', figsize=(15, 10), title=f'Bar Chart for {metric.capitalize()}')\n",
        "    plt.ylabel(metric)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Classifiers', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_bar_charts(results_df, metric)\n",
        "\n",
        "# 4iv) Line Graphs\n",
        "def plot_line_graphs(results_df, metric):\n",
        "    results_metric = results_df[metric].unstack(level=1)\n",
        "    results_metric.plot(kind='line', marker='o', figsize=(15, 10), title=f'Line Graph for {metric.capitalize()}')\n",
        "    plt.ylabel(metric)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Classifiers', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_line_graphs(results_df, metric)\n",
        "\n",
        "# 4vi) Box Plots\n",
        "def plot_box_plots(results_df, metric):\n",
        "    results_metric = results_df[metric].unstack(level=1)\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    sns.boxplot(data=results_metric)\n",
        "    plt.title(f'Box Plot for {metric.capitalize()}')\n",
        "    plt.ylabel(metric)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    plot_box_plots(results_df, metric)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}